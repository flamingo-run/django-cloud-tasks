{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Django Cloud Tasks! Django Cloud Tasks is your go-to Django app for effortlessly running asynchronous tasks on Google Cloud Platform. It makes it a breeze to work with Google Cloud Tasks, Cloud Scheduler, and Cloud Pub/Sub right from your Django project. Think of it as a way to offload heavy work (like image processing or report generation), schedule future jobs (like nightly cleanups), or react to events in a decoupled way (like sending a welcome email when a new user signs up) \u2013 all without making your users wait or bogging down your web servers. This documentation will guide you through setting up and using its powerful features. Installation Getting this bad boy into your project is super easy. Just pip install it: pip install django-cloud-tasks Django Setup Add 'django_cloud_tasks' to your INSTALLED_APPS in settings.py : python INSTALLED_APPS = [ # ... other apps 'django_cloud_tasks', # ... ] Include the Django Cloud Tasks URLs in your main urls.py . These URLs are the endpoints that Google Cloud services will call to trigger your tasks. ```python from django.urls import path, include urlpatterns = [ # ... other urls path('my-tasks-prefix/', include('django_cloud_tasks.urls')), # You can choose your own prefix # ... ] ``` Make sure this endpoint is publicly accessible if you're not running in a private VPC, as Google Cloud services need to reach it. Required Google Cloud APIs To use django-cloud-tasks effectively, you'll need to enable the following APIs in your Google Cloud Project: Cloud Tasks API Cloud Scheduler API Pub/Sub API Optional: Admin SDK API (Needed for some advanced features or if you are working with domain-wide delegation for service accounts, though typically not required for basic task and pub/sub operations with OIDC or standard service account authentication). Core Configuration ( settings.py ) You can tweak how Django Cloud Tasks behaves through your Django settings.py file. All settings are prefixed with DJANGO_CLOUD_TASKS_ . You can also set these as environment variables (which will take precedence if both are set). Here are some of the main ones to get you started: DJANGO_CLOUD_TASKS_ENDPOINT : The full base URL of your application (e.g., https://your-cool-app.com ). This is crucial because Cloud Tasks, Scheduler, and Pub/Sub push subscriptions need to know the exact URL to send HTTP requests to trigger your tasks. It's often your Cloud Run service URL. Default: \"http://localhost:8080\" Example: DJANGO_CLOUD_TASKS_ENDPOINT = \"https://myapp.com\" DJANGO_CLOUD_TASKS_APP_NAME : A unique name for your application or service. This is used to prefix and organize resources in GCP, such as Cloud Scheduler job names or Pub/Sub topic/subscription names, making it easier to manage them, especially if you have multiple applications in the same GCP project. Default: None (reads from APP_NAME environment variable if set) Example: DJANGO_CLOUD_TASKS_APP_NAME = \"user-service\" DJANGO_CLOUD_TASKS_EAGER : If set to True , tasks will run synchronously (i.e., immediately in the same process) instead of being sent to Google Cloud. This is incredibly useful for local development and testing, as it bypasses the need for GCP setup and lets you debug tasks like regular function calls. Default: False Example: DJANGO_CLOUD_TASKS_EAGER = settings.DEBUG (to enable eager mode when Django's DEBUG is true) DJANGO_CLOUD_TASKS_URL_NAME : The specific Django URL name (not path) within the included django_cloud_tasks.urls that is used as the endpoint for on-demand tasks triggered by Cloud Tasks and scheduled tasks triggered by Cloud Scheduler. Default: \"tasks-endpoint\" DJANGO_CLOUD_TASKS_DELIMITER : A string used to join parts of names, for example, when constructing default queue names, scheduler job names, or Pub/Sub topic/subscription names (e.g., my-app--my-task ). Default: \"--\" Header Propagation : Settings related to header propagation are covered in detail in the \"Header Propagation\" section. There are more settings for fine-tuning retries, Pub/Sub behavior, and specific service interactions. We'll touch upon many of these in the relevant sections. For a comprehensive list, you can always refer to the django_cloud_tasks.apps.DjangoCloudTasksAppConfig class. With this foundational setup, you're ready to dive into defining and using the different types of tasks!","title":"Getting Started"},{"location":"#welcome-to-django-cloud-tasks","text":"Django Cloud Tasks is your go-to Django app for effortlessly running asynchronous tasks on Google Cloud Platform. It makes it a breeze to work with Google Cloud Tasks, Cloud Scheduler, and Cloud Pub/Sub right from your Django project. Think of it as a way to offload heavy work (like image processing or report generation), schedule future jobs (like nightly cleanups), or react to events in a decoupled way (like sending a welcome email when a new user signs up) \u2013 all without making your users wait or bogging down your web servers. This documentation will guide you through setting up and using its powerful features.","title":"Welcome to Django Cloud Tasks!"},{"location":"#installation","text":"Getting this bad boy into your project is super easy. Just pip install it: pip install django-cloud-tasks","title":"Installation"},{"location":"#django-setup","text":"Add 'django_cloud_tasks' to your INSTALLED_APPS in settings.py : python INSTALLED_APPS = [ # ... other apps 'django_cloud_tasks', # ... ] Include the Django Cloud Tasks URLs in your main urls.py . These URLs are the endpoints that Google Cloud services will call to trigger your tasks. ```python from django.urls import path, include urlpatterns = [ # ... other urls path('my-tasks-prefix/', include('django_cloud_tasks.urls')), # You can choose your own prefix # ... ] ``` Make sure this endpoint is publicly accessible if you're not running in a private VPC, as Google Cloud services need to reach it.","title":"Django Setup"},{"location":"#required-google-cloud-apis","text":"To use django-cloud-tasks effectively, you'll need to enable the following APIs in your Google Cloud Project: Cloud Tasks API Cloud Scheduler API Pub/Sub API Optional: Admin SDK API (Needed for some advanced features or if you are working with domain-wide delegation for service accounts, though typically not required for basic task and pub/sub operations with OIDC or standard service account authentication).","title":"Required Google Cloud APIs"},{"location":"#core-configuration-settingspy","text":"You can tweak how Django Cloud Tasks behaves through your Django settings.py file. All settings are prefixed with DJANGO_CLOUD_TASKS_ . You can also set these as environment variables (which will take precedence if both are set). Here are some of the main ones to get you started: DJANGO_CLOUD_TASKS_ENDPOINT : The full base URL of your application (e.g., https://your-cool-app.com ). This is crucial because Cloud Tasks, Scheduler, and Pub/Sub push subscriptions need to know the exact URL to send HTTP requests to trigger your tasks. It's often your Cloud Run service URL. Default: \"http://localhost:8080\" Example: DJANGO_CLOUD_TASKS_ENDPOINT = \"https://myapp.com\" DJANGO_CLOUD_TASKS_APP_NAME : A unique name for your application or service. This is used to prefix and organize resources in GCP, such as Cloud Scheduler job names or Pub/Sub topic/subscription names, making it easier to manage them, especially if you have multiple applications in the same GCP project. Default: None (reads from APP_NAME environment variable if set) Example: DJANGO_CLOUD_TASKS_APP_NAME = \"user-service\" DJANGO_CLOUD_TASKS_EAGER : If set to True , tasks will run synchronously (i.e., immediately in the same process) instead of being sent to Google Cloud. This is incredibly useful for local development and testing, as it bypasses the need for GCP setup and lets you debug tasks like regular function calls. Default: False Example: DJANGO_CLOUD_TASKS_EAGER = settings.DEBUG (to enable eager mode when Django's DEBUG is true) DJANGO_CLOUD_TASKS_URL_NAME : The specific Django URL name (not path) within the included django_cloud_tasks.urls that is used as the endpoint for on-demand tasks triggered by Cloud Tasks and scheduled tasks triggered by Cloud Scheduler. Default: \"tasks-endpoint\" DJANGO_CLOUD_TASKS_DELIMITER : A string used to join parts of names, for example, when constructing default queue names, scheduler job names, or Pub/Sub topic/subscription names (e.g., my-app--my-task ). Default: \"--\" Header Propagation : Settings related to header propagation are covered in detail in the \"Header Propagation\" section. There are more settings for fine-tuning retries, Pub/Sub behavior, and specific service interactions. We'll touch upon many of these in the relevant sections. For a comprehensive list, you can always refer to the django_cloud_tasks.apps.DjangoCloudTasksAppConfig class. With this foundational setup, you're ready to dive into defining and using the different types of tasks!","title":"Core Configuration (settings.py)"},{"location":"headers/","text":"Header Propagation When you're dealing with asynchronous tasks, especially in a microservices or distributed environment, it's often crucial to carry over some context from the initial request that triggered the task. Header propagation allows you to do just that. Why Propagate Headers? Common use cases include: Distributed Tracing: To track a single logical operation as it flows through multiple services or asynchronous tasks. Headers like traceparent (as used by OpenTelemetry and others) or X-Cloud-Trace-Context (used by Google Cloud) are prime candidates. Tenant/User Context: If you have a multi-tenant application, you might want to propagate a X-Tenant-ID or X-User-ID so the task execution environment knows which tenant's data to operate on or which user initiated the action (primarily for logging or non-security-sensitive context). Feature Flags or A/B Testing Context: Propagating headers related to feature flags or A/B testing variants can ensure consistent behavior in asynchronous tasks. Client Information: Propagating User-Agent or custom client version headers ( X-Client-Version ) for logging and debugging purposes. How It Works Header propagation relies on middleware to manage context: Capture: The HeadersContextMiddleware intercepts incoming Django requests, extracts headers specified in DJANGO_CLOUD_TASKS_PROPAGATED_HEADERS , and stores them in a request-local context. Forwarding: Cloud Tasks (On-Demand/Scheduled): When a task is pushed, these stored headers are added as HTTP headers to the request Cloud Tasks (or Cloud Scheduler) makes to your application. Pub/Sub (PublisherTask): Propagated headers are embedded as a dictionary within the JSON payload of the Pub/Sub message, under the key defined by DJANGO_CLOUD_TASKS_PROPAGATED_HEADERS_KEY . Retrieval: Cloud Tasks: Within your Task or PeriodicTask , self._metadata.custom_headers provides access to these propagated HTTP headers. Pub/Sub: The PubSubHeadersMiddleware extracts the embedded headers from the message payload when a push request for a SubscriberTask arrives. Your SubscriberTask then typically accesses them from the message content dictionary. Configuration 1. Middleware Setup To enable header propagation, you must add the relevant middleware to your settings.py : MIDDLEWARE = [ # ... other django middleware ... 'django_cloud_tasks.middleware.HeadersContextMiddleware', 'django_cloud_tasks.middleware.PubSubHeadersMiddleware', # Important for subscriber tasks # ... other app middleware ... ] HeadersContextMiddleware : Essential for capturing headers from incoming Django requests and making them available for propagation when new tasks are initiated. It also helps make headers available within a task's execution if they were propagated by Cloud Tasks. PubSubHeadersMiddleware : Specifically for tasks triggered by Pub/Sub push subscriptions. It extracts headers that were embedded in the Pub/Sub message body by a PublisherTask and loads them into the Django request context for the subscriber task handler. 2. Settings for Propagation You configure which headers are propagated and how they are keyed in Pub/Sub messages via your Django settings.py : DJANGO_CLOUD_TASKS_PROPAGATED_HEADERS : A list of HTTP header names that you want to capture and propagate. The matching from incoming requests is case-insensitive. Default: [\"traceparent\"] Example: python DJANGO_CLOUD_TASKS_PROPAGATED_HEADERS = [ \"traceparent\", \"X-Request-ID\", \"X-Tenant-ID\", \"X-User-ID\", \"Accept-Language\", ] DJANGO_CLOUD_TASKS_PROPAGATED_HEADERS_KEY (For Pub/Sub PublisherTask / SubscriberTask ): This setting defines the key within the JSON message body where the dictionary of propagated headers will be stored by PublisherTask and read from by PubSubHeadersMiddleware (and your SubscriberTask ). Default: \"_http_headers\" Example: python DJANGO_CLOUD_TASKS_PROPAGATED_HEADERS_KEY = \"_propagated_context_headers\" Accessing Propagated Headers in Your Tasks 1. For On-Demand ( Task ) and Scheduled ( PeriodicTask ) Tasks These tasks are executed via an HTTP request from Google Cloud Tasks/Scheduler. Propagated headers are part of this request. Access them via self._metadata.custom_headers in your task's run method. This dictionary holds headers from the task execution request that were listed in DJANGO_CLOUD_TASKS_PROPAGATED_HEADERS . from django_cloud_tasks.tasks import Task class ProcessDataWithTraceTask(Task): def run(self, data_id: int): # Headers in custom_headers are typically lowercased trace_id = self._metadata.custom_headers.get(\"traceparent\") tenant_id = self._metadata.custom_headers.get(\"x-tenant-id\") print(f\"Executing for data ID: {data_id}, Trace: {trace_id}, Tenant: {tenant_id}\") # ... your task logic ... 2. For Subscriber Tasks ( SubscriberTask - Pub/Sub) With PublisherTask , headers are embedded in the Pub/Sub message body . Your SubscriberTask accesses them from the content dictionary using the DJANGO_CLOUD_TASKS_PROPAGATED_HEADERS_KEY . from django_cloud_tasks.tasks import SubscriberTask from django_cloud_tasks.tasks.helpers import get_app class AuditLogSubscriber(SubscriberTask): @classmethod def topic_name(cls) -> str: return \"user-activity-events\" def run(self, content: dict, attributes: dict[str, str] | None = None): headers_key = get_app().propagated_headers_key propagated_headers = content.get(headers_key, {}) user_id = propagated_headers.get(\"X-User-ID\") # Case here matches what was put into the dict request_id = propagated_headers.get(\"X-Request-ID\") # original_event_payload might exclude the headers for clarity if needed # original_event_payload = {k: v for k, v in content.items() if k != headers_key} print(f\"Audit event: {content}, User: {user_id}, Request: {request_id}\") # ... audit logging ... By correctly setting up the middleware and configurations, header propagation provides valuable context for your asynchronous operations.","title":"Header Propagation"},{"location":"headers/#header-propagation","text":"When you're dealing with asynchronous tasks, especially in a microservices or distributed environment, it's often crucial to carry over some context from the initial request that triggered the task. Header propagation allows you to do just that.","title":"Header Propagation"},{"location":"headers/#why-propagate-headers","text":"Common use cases include: Distributed Tracing: To track a single logical operation as it flows through multiple services or asynchronous tasks. Headers like traceparent (as used by OpenTelemetry and others) or X-Cloud-Trace-Context (used by Google Cloud) are prime candidates. Tenant/User Context: If you have a multi-tenant application, you might want to propagate a X-Tenant-ID or X-User-ID so the task execution environment knows which tenant's data to operate on or which user initiated the action (primarily for logging or non-security-sensitive context). Feature Flags or A/B Testing Context: Propagating headers related to feature flags or A/B testing variants can ensure consistent behavior in asynchronous tasks. Client Information: Propagating User-Agent or custom client version headers ( X-Client-Version ) for logging and debugging purposes.","title":"Why Propagate Headers?"},{"location":"headers/#how-it-works","text":"Header propagation relies on middleware to manage context: Capture: The HeadersContextMiddleware intercepts incoming Django requests, extracts headers specified in DJANGO_CLOUD_TASKS_PROPAGATED_HEADERS , and stores them in a request-local context. Forwarding: Cloud Tasks (On-Demand/Scheduled): When a task is pushed, these stored headers are added as HTTP headers to the request Cloud Tasks (or Cloud Scheduler) makes to your application. Pub/Sub (PublisherTask): Propagated headers are embedded as a dictionary within the JSON payload of the Pub/Sub message, under the key defined by DJANGO_CLOUD_TASKS_PROPAGATED_HEADERS_KEY . Retrieval: Cloud Tasks: Within your Task or PeriodicTask , self._metadata.custom_headers provides access to these propagated HTTP headers. Pub/Sub: The PubSubHeadersMiddleware extracts the embedded headers from the message payload when a push request for a SubscriberTask arrives. Your SubscriberTask then typically accesses them from the message content dictionary.","title":"How It Works"},{"location":"headers/#configuration","text":"","title":"Configuration"},{"location":"headers/#1-middleware-setup","text":"To enable header propagation, you must add the relevant middleware to your settings.py : MIDDLEWARE = [ # ... other django middleware ... 'django_cloud_tasks.middleware.HeadersContextMiddleware', 'django_cloud_tasks.middleware.PubSubHeadersMiddleware', # Important for subscriber tasks # ... other app middleware ... ] HeadersContextMiddleware : Essential for capturing headers from incoming Django requests and making them available for propagation when new tasks are initiated. It also helps make headers available within a task's execution if they were propagated by Cloud Tasks. PubSubHeadersMiddleware : Specifically for tasks triggered by Pub/Sub push subscriptions. It extracts headers that were embedded in the Pub/Sub message body by a PublisherTask and loads them into the Django request context for the subscriber task handler.","title":"1. Middleware Setup"},{"location":"headers/#2-settings-for-propagation","text":"You configure which headers are propagated and how they are keyed in Pub/Sub messages via your Django settings.py : DJANGO_CLOUD_TASKS_PROPAGATED_HEADERS : A list of HTTP header names that you want to capture and propagate. The matching from incoming requests is case-insensitive. Default: [\"traceparent\"] Example: python DJANGO_CLOUD_TASKS_PROPAGATED_HEADERS = [ \"traceparent\", \"X-Request-ID\", \"X-Tenant-ID\", \"X-User-ID\", \"Accept-Language\", ] DJANGO_CLOUD_TASKS_PROPAGATED_HEADERS_KEY (For Pub/Sub PublisherTask / SubscriberTask ): This setting defines the key within the JSON message body where the dictionary of propagated headers will be stored by PublisherTask and read from by PubSubHeadersMiddleware (and your SubscriberTask ). Default: \"_http_headers\" Example: python DJANGO_CLOUD_TASKS_PROPAGATED_HEADERS_KEY = \"_propagated_context_headers\"","title":"2. Settings for Propagation"},{"location":"headers/#accessing-propagated-headers-in-your-tasks","text":"","title":"Accessing Propagated Headers in Your Tasks"},{"location":"headers/#1-for-on-demand-task-and-scheduled-periodictask-tasks","text":"These tasks are executed via an HTTP request from Google Cloud Tasks/Scheduler. Propagated headers are part of this request. Access them via self._metadata.custom_headers in your task's run method. This dictionary holds headers from the task execution request that were listed in DJANGO_CLOUD_TASKS_PROPAGATED_HEADERS . from django_cloud_tasks.tasks import Task class ProcessDataWithTraceTask(Task): def run(self, data_id: int): # Headers in custom_headers are typically lowercased trace_id = self._metadata.custom_headers.get(\"traceparent\") tenant_id = self._metadata.custom_headers.get(\"x-tenant-id\") print(f\"Executing for data ID: {data_id}, Trace: {trace_id}, Tenant: {tenant_id}\") # ... your task logic ...","title":"1. For On-Demand (Task) and Scheduled (PeriodicTask) Tasks"},{"location":"headers/#2-for-subscriber-tasks-subscribertask-pubsub","text":"With PublisherTask , headers are embedded in the Pub/Sub message body . Your SubscriberTask accesses them from the content dictionary using the DJANGO_CLOUD_TASKS_PROPAGATED_HEADERS_KEY . from django_cloud_tasks.tasks import SubscriberTask from django_cloud_tasks.tasks.helpers import get_app class AuditLogSubscriber(SubscriberTask): @classmethod def topic_name(cls) -> str: return \"user-activity-events\" def run(self, content: dict, attributes: dict[str, str] | None = None): headers_key = get_app().propagated_headers_key propagated_headers = content.get(headers_key, {}) user_id = propagated_headers.get(\"X-User-ID\") # Case here matches what was put into the dict request_id = propagated_headers.get(\"X-Request-ID\") # original_event_payload might exclude the headers for clarity if needed # original_event_payload = {k: v for k, v in content.items() if k != headers_key} print(f\"Audit event: {content}, User: {user_id}, Request: {request_id}\") # ... audit logging ... By correctly setting up the middleware and configurations, header propagation provides valuable context for your asynchronous operations.","title":"2. For Subscriber Tasks (SubscriberTask - Pub/Sub)"},{"location":"on_demand_tasks/","text":"On-Demand Tasks (Powered by Google Cloud Tasks) On-demand tasks are your workhorses for anything you want to do asynchronously. Think of sending a welcome email after a user signs up, processing an image after upload, or calling a slow external API without making your user wait. These tasks are pushed to a Google Cloud Tasks queue and executed by an HTTP request back to your Django application. Defining a Basic Task Creating a task is as simple as inheriting from django_cloud_tasks.tasks.Task and implementing the run method. This method contains the logic your task will execute. Example: Sending a Welcome Email Let's say you want to send a welcome email to a new user. Their details (like email and name) will be passed as arguments to the task. # In your app's tasks.py (e.g., users/tasks.py) from django.contrib.auth.models import User from django.core.mail import send_mail from django_cloud_tasks.tasks import Task class SendWelcomeEmailTask(Task): def run(self, user_id: int, custom_message: str | None = None): try: user = User.objects.get(pk=user_id) subject = f\"Welcome to Our Awesome Platform, {user.first_name}!\" message_body = custom_message or f\"Hi {user.first_name}, \\n\\nThanks for signing up! We're thrilled to have you.\" send_mail( subject=subject, message=message_body, from_email=\"noreply@myawesomeplatform.com\", recipient_list=[user.email], fail_silently=False, ) print(f\"Welcome email sent to {user.email}\") return {\"status\": \"success\", \"user_id\": user_id, \"email\": user.email} except User.DoesNotExist: print(f\"User with ID {user_id} not found. Cannot send welcome email.\") # You might want to raise an exception here to have Cloud Tasks retry, or handle it as a permanent failure. return {\"status\": \"error\", \"reason\": \"user_not_found\"} except Exception as e: print(f\"Failed to send welcome email to user {user_id}: {e}\") # Re-raise the exception to make Cloud Tasks retry based on queue configuration raise Important Notes: Task Discovery: Django Cloud Tasks automatically discovers task classes that inherit from Task , PeriodicTask , etc. Just ensure the Python modules containing your task definitions (e.g., myapp/tasks.py ) are imported by Django at startup. A common pattern is to import them in your app's apps.py within the ready() method. Serialization: Arguments passed to your task's run method (and the return value) must be JSON serializable . Basic types like integers, strings, lists, and dicts are fine. For complex objects like Django model instances, you should pass identifiers (like a primary key) and re-fetch the object within the task, as shown in the SendWelcomeEmailTask . Running Your Tasks Once defined, you can trigger your tasks from anywhere in your Django code (views, signals, model methods, etc.). asap(**kwargs) - Run As Soon As Possible: This is the most common way. It enqueues the task to be executed by Cloud Tasks as soon as a worker is available. The kwargs are the arguments to your task's run method. ```python Assuming user_just_registered is a User object SendWelcomeEmailTask.asap(user_id=user_just_registered.id) SendWelcomeEmailTask.asap(user_id=another_user.id, custom_message=\"Hey there, special user!\") ``` later(task_kwargs: dict, eta: int | timedelta | datetime) - Schedule for a Specific Future Time: Use this to delay a task's execution. ```python from datetime import timedelta from django.utils import timezone Send a follow-up email 7 days after signup user_id_to_follow_up = 123 follow_up_time = timezone.now() + timedelta(days=7) SendWelcomeEmailTask.later( task_kwargs={\"user_id\": user_id_to_follow_up, \"custom_message\": \"Hope you're enjoying the platform after your first week!\"}, eta=follow_up_time ) Or run 30 minutes from now (eta can be seconds as an int) SendWelcomeEmailTask.later(task_kwargs={\"user_id\": 456}, eta=30 * 60) ``` When using eta , be mindful of the DJANGO_CLOUD_TASKS_MAXIMUM_ETA_TASK setting (or its environment variable counterpart). This setting defines the maximum number of seconds into the future a task can be scheduled. Google Cloud Tasks itself has a limit (typically 30 days). If DJANGO_CLOUD_TASKS_MAXIMUM_ETA_TASK is set, it imposes an additional, potentially more restrictive, limit within your application. * Default: None (meaning the library doesn't impose its own limit beyond GCP's). * Example: DJANGO_CLOUD_TASKS_MAXIMUM_ETA_TASK = 60 * 60 * 24 * 7 (limit to 7 days). sync(**kwargs) - Run Synchronously (Mainly for Testing & Eager Mode): Executes the task's run method immediately in the current process. This is the default behavior if DJANGO_CLOUD_TASKS_EAGER = True is set in your settings. It's extremely helpful for debugging and local development. ```python This will run the SendWelcomeEmailTask.run() method directly result = SendWelcomeEmailTask.sync(user_id=some_user.id, custom_message=\"Testing sync call.\") print(result) # Output: {'status': 'success', 'user_id': ..., 'email': ...} ``` until(task_kwargs: dict, max_eta: datetime) - Schedule Randomly Before a Deadline: Schedules the task to run at a random time between now and the specified max_eta (maximum execution time). Useful for distributing load for non-time-critical tasks. It's not a load balancer, it's just a way to schedule tasks to run at different times. ```python from django.utils import timezone from datetime import timedelta Process a batch of optional analytics updates sometime in the next hour max_execution_time = timezone.now() + timedelta(hours=1) class ProcessAnalyticsBatch(Task): def run(self, user_ids: list[int]): for user_id in user_ids: SendWelcomeEmailTask.until(task_kwargs={\"user_id\": user_id}, max_eta=max_execution_time) ``` Advanced Task Configuration You can fine-tune task behavior using several class attributes and methods on your Task subclass. These configurations often map to Google Cloud Tasks API features. Ensuring Uniqueness: only_once Set only_once = True on your task class if you want to prevent duplicate enqueues of tasks that are identical by class name for a given queue. Cloud Tasks will use a deterministic task name derived from your task's class name. Use Case: Imagine you have a task ProcessOrderTask that gets called when an order is submitted. If, due to a client-side retry or a race condition, your system tries to enqueue ProcessOrderTask for the same order multiple times in quick succession before the first one is picked up , only_once = True can help prevent multiple identical tasks for that order from being added to the queue initially. Caveat: This de-duplication is based on the task class name . It doesn't consider task arguments. If you need de-duplication based on specific arguments (e.g., \"only one ProcessOrderTask for order_id=123 \"), you'd need to implement that logic yourself, perhaps by checking for an existing task with a custom-generated name via the push() method, or within your run method. class ProcessOrderTask(Task): only_once = True # Prevents rapid, identical enqueues by class name def run(self, order_id: int, payment_details: dict): print(f\"Processing order {order_id}...\") # ... order processing logic ... Enqueue Retry Policy These settings control retries if the the act of trying to send the task to Google Cloud Tasks fails, for example, due to a transient network issue or a temporary problem with the Cloud Tasks API. This is not about retrying your task if its run() method fails. enqueue_retry_exceptions: list[str | Type[Exception]] | None : A list of exception types (or their string paths like 'google.api_core.exceptions.ServiceUnavailable' ) that should trigger a retry. Defaults to an empty list (or what's set globally via DJANGO_CLOUD_TASKS_ENQUEUE_RETRY_EXCEPTIONS ). enqueue_retry_initial: float | None : Initial delay in seconds for the first retry. Defaults to global config. enqueue_retry_maximum: float | None : Maximum delay in seconds between retries. Defaults to global config. enqueue_retry_multiplier: float | None : Multiplier for increasing the delay between retries. Defaults to global config. enqueue_retry_deadline: float | None : Total time in seconds to keep retrying. Defaults to global config. from google.api_core.exceptions import ServiceUnavailable, InternalServerError class RobustEnqueueTask(Task): enqueue_retry_exceptions = [ServiceUnavailable, InternalServerError, \"requests.exceptions.ConnectionError\"] enqueue_retry_initial = 2.0 # Start with a 2-second delay enqueue_retry_maximum = 60.0 # Cap retries at 1 minute intervals enqueue_retry_multiplier = 2.5 enqueue_retry_deadline = 300.0 # Try for up to 5 minutes def run(self, data: dict): print(\"Task enqueued robustly and now running.\") Using Custom Queue Names By default, tasks are sent to a queue named after your DJANGO_CLOUD_TASKS_APP_NAME (or just \"tasks\" if APP_NAME is not set). You can direct specific tasks to different queues by overriding the queue() class method. This is useful for prioritizing tasks or managing different workloads (e.g., short, quick tasks vs. long-running batch jobs). Make sure any custom queues you specify actually exist in your Google Cloud Tasks project. class HighPriorityNotification(Task): @classmethod def queue(cls) -> str: return \"critical-notifications-queue\" # Ensure this queue is configured in GCP with appropriate retries etc. def run(self, user_id: int, alert_message: str): # ... send an urgent alert ... print(f\"Sent high priority alert to user {user_id}\") class BatchDataProcessing(Task): @classmethod def queue(cls) -> str: return \"batch-jobs-queue\" # A queue for longer, less time-sensitive tasks def run(self, dataset_id: str): # ... process a large dataset ... print(f\"Processed batch data for {dataset_id}\") Suggesting a Task Timeout Cloud Tasks need to know how long to wait for your task to complete before considering it timed out. You can suggest a timeout by overriding get_task_timeout() . The actual timeout is ultimately controlled by the queue configuration in GCP, but this provides a hint. from datetime import timedelta class ReportGenerationTask(Task): @classmethod def get_task_timeout(cls) -> timedelta | None: # This report can sometimes take a while return timedelta(minutes=15) # Suggest Cloud Tasks allow up to 15 minutes def run(self, report_params: dict): # ... logic to generate a potentially long report ... print(\"Report generation complete.\") Accessing Task Metadata ( self._metadata ) Inside your task's run method, you can access information about the current execution attempt via self._metadata . This is an instance of TaskMetadata (or your custom class specified by DJANGO_CLOUD_TASKS_TASK_METADATA_CLASS ). Use Case: You might want to log the attempt number, or have slightly different behavior on the first attempt versus a retry (e.g., more aggressive external API calls on first try, then back off). class RetryAwareTask(Task): def run(self, api_call_details: dict): print(f\"--- Task Execution --- \") print(f\"Task ID: {self._metadata.task_id}\") print(f\"Queue: {self._metadata.queue_name}\") print(f\"Attempt Number (dispatch count + 1): {self._metadata.attempt_number}\") print(f\"Execution Number (non-5XX responses): {self._metadata.execution_number}\") print(f\"Scheduled ETA: {self._metadata.eta}\") if self._metadata.previous_failure: print(f\"This task previously failed with reason: {self._metadata.previous_failure}\") if self._metadata.is_cloud_scheduler: print(f\"This task was triggered by Cloud Scheduler job: {self._metadata.cloud_scheduler_job_name}\") # Example: Different logic for first attempt vs retries if self._metadata.first_attempt: print(\"First attempt: trying the primary API endpoint.\") # make_api_call(api_call_details, endpoint_type=\"primary\") else: print(\"Retry attempt: trying a fallback API endpoint or with reduced payload.\") # make_api_call(api_call_details, endpoint_type=\"fallback\") # Your main task logic here # ... Using push() for Full Control For the most granular control over task enqueuing, you can use the push() class method directly. asap() and later() are convenient wrappers around push() . This allows you to specify custom headers, a dynamically generated task name (useful for more specific de-duplication than only_once ), or override the queue dynamically. # Generate a unique task name for de-duplication based on arguments # This requires more careful management than `only_once = True` unique_part = hashlib.md5(f\"{user_id}-{item_id}\".encode()).hexdigest() deterministic_task_name = f\"ProcessItemTask-{unique_part}\" SendWelcomeEmailTask.push( task_kwargs={\"user_id\": 777, \"custom_message\": \"Pushed with full control!\"}, headers={\"X-Correlation-ID\": \"my-custom-trace-id\"}, # Custom headers for this specific push delay_in_seconds=60, # Delay by 1 minute queue=\"another-dynamic-queue\", # Can override queue here too task_name=deterministic_task_name, # Provide a specific task name ) Managing and Debugging Tasks Django Cloud Tasks provides a couple of utility methods on your task classes for interacting with tasks already in a queue . Debugging a Specific Task ( debug() ) If a task has failed in the cloud, or you want to re-run a specific execution locally with the exact same payload it had, you can use debug() . How it works: It fetches the task details (including its original payload) from Google Cloud Tasks using its task_id and then executes the run() method locally (synchronously) with that payload. It also populates self._metadata from the fetched task information. # In a Django shell or management command: # from myapp.tasks import SendWelcomeEmailTask # Assume you got this task ID from your GCP logs task_id_from_gcp = \"1234567890abcdef\" # This will fetch the task, deserialize its payload, and run it locally: SendWelcomeEmailTask.debug(task_id=task_id_from_gcp) Discarding Tasks ( discard() ) You can remove tasks from a Google Cloud Tasks queue. Discard a specific task by ID: python # SendWelcomeEmailTask.discard(task_id=\"projects/.../tasks/someSpecificId\") Discard all tasks of a certain type from a queue (use with caution!): python # This will list all tasks for SendWelcomeEmailTask in its default queue and delete them. # deleted_tasks_info = SendWelcomeEmailTask.discard() # print(f\"Deleted tasks: {deleted_tasks_info}\") Discard tasks of a certain type that have met a minimum retry count: Useful for clearing out tasks that seem to be failing persistently after a certain number of attempts. python # Delete all SendWelcomeEmailTask instances from the queue that have been dispatched (retried) at least 5 times. # SendWelcomeEmailTask.discard(min_retries=5) Handling Failed Tasks Any exception raised by your task's run() method is considered a failure. This will inform Google Cloud Tasks to retry the task, according to the policy you've set. If you want to skip retrying a task, you can raise DiscardTaskException from within your task. This covers the ins and outs of on-demand tasks. They form the foundation for many asynchronous operations in your Django application.","title":"On-Demand Tasks"},{"location":"on_demand_tasks/#on-demand-tasks-powered-by-google-cloud-tasks","text":"On-demand tasks are your workhorses for anything you want to do asynchronously. Think of sending a welcome email after a user signs up, processing an image after upload, or calling a slow external API without making your user wait. These tasks are pushed to a Google Cloud Tasks queue and executed by an HTTP request back to your Django application.","title":"On-Demand Tasks (Powered by Google Cloud Tasks)"},{"location":"on_demand_tasks/#defining-a-basic-task","text":"Creating a task is as simple as inheriting from django_cloud_tasks.tasks.Task and implementing the run method. This method contains the logic your task will execute. Example: Sending a Welcome Email Let's say you want to send a welcome email to a new user. Their details (like email and name) will be passed as arguments to the task. # In your app's tasks.py (e.g., users/tasks.py) from django.contrib.auth.models import User from django.core.mail import send_mail from django_cloud_tasks.tasks import Task class SendWelcomeEmailTask(Task): def run(self, user_id: int, custom_message: str | None = None): try: user = User.objects.get(pk=user_id) subject = f\"Welcome to Our Awesome Platform, {user.first_name}!\" message_body = custom_message or f\"Hi {user.first_name}, \\n\\nThanks for signing up! We're thrilled to have you.\" send_mail( subject=subject, message=message_body, from_email=\"noreply@myawesomeplatform.com\", recipient_list=[user.email], fail_silently=False, ) print(f\"Welcome email sent to {user.email}\") return {\"status\": \"success\", \"user_id\": user_id, \"email\": user.email} except User.DoesNotExist: print(f\"User with ID {user_id} not found. Cannot send welcome email.\") # You might want to raise an exception here to have Cloud Tasks retry, or handle it as a permanent failure. return {\"status\": \"error\", \"reason\": \"user_not_found\"} except Exception as e: print(f\"Failed to send welcome email to user {user_id}: {e}\") # Re-raise the exception to make Cloud Tasks retry based on queue configuration raise Important Notes: Task Discovery: Django Cloud Tasks automatically discovers task classes that inherit from Task , PeriodicTask , etc. Just ensure the Python modules containing your task definitions (e.g., myapp/tasks.py ) are imported by Django at startup. A common pattern is to import them in your app's apps.py within the ready() method. Serialization: Arguments passed to your task's run method (and the return value) must be JSON serializable . Basic types like integers, strings, lists, and dicts are fine. For complex objects like Django model instances, you should pass identifiers (like a primary key) and re-fetch the object within the task, as shown in the SendWelcomeEmailTask .","title":"Defining a Basic Task"},{"location":"on_demand_tasks/#running-your-tasks","text":"Once defined, you can trigger your tasks from anywhere in your Django code (views, signals, model methods, etc.). asap(**kwargs) - Run As Soon As Possible: This is the most common way. It enqueues the task to be executed by Cloud Tasks as soon as a worker is available. The kwargs are the arguments to your task's run method. ```python","title":"Running Your Tasks"},{"location":"on_demand_tasks/#assuming-user_just_registered-is-a-user-object","text":"SendWelcomeEmailTask.asap(user_id=user_just_registered.id) SendWelcomeEmailTask.asap(user_id=another_user.id, custom_message=\"Hey there, special user!\") ``` later(task_kwargs: dict, eta: int | timedelta | datetime) - Schedule for a Specific Future Time: Use this to delay a task's execution. ```python from datetime import timedelta from django.utils import timezone","title":"Assuming user_just_registered is a User object"},{"location":"on_demand_tasks/#send-a-follow-up-email-7-days-after-signup","text":"user_id_to_follow_up = 123 follow_up_time = timezone.now() + timedelta(days=7) SendWelcomeEmailTask.later( task_kwargs={\"user_id\": user_id_to_follow_up, \"custom_message\": \"Hope you're enjoying the platform after your first week!\"}, eta=follow_up_time )","title":"Send a follow-up email 7 days after signup"},{"location":"on_demand_tasks/#or-run-30-minutes-from-now-eta-can-be-seconds-as-an-int","text":"SendWelcomeEmailTask.later(task_kwargs={\"user_id\": 456}, eta=30 * 60) ``` When using eta , be mindful of the DJANGO_CLOUD_TASKS_MAXIMUM_ETA_TASK setting (or its environment variable counterpart). This setting defines the maximum number of seconds into the future a task can be scheduled. Google Cloud Tasks itself has a limit (typically 30 days). If DJANGO_CLOUD_TASKS_MAXIMUM_ETA_TASK is set, it imposes an additional, potentially more restrictive, limit within your application. * Default: None (meaning the library doesn't impose its own limit beyond GCP's). * Example: DJANGO_CLOUD_TASKS_MAXIMUM_ETA_TASK = 60 * 60 * 24 * 7 (limit to 7 days). sync(**kwargs) - Run Synchronously (Mainly for Testing & Eager Mode): Executes the task's run method immediately in the current process. This is the default behavior if DJANGO_CLOUD_TASKS_EAGER = True is set in your settings. It's extremely helpful for debugging and local development. ```python","title":"Or run 30 minutes from now (eta can be seconds as an int)"},{"location":"on_demand_tasks/#this-will-run-the-sendwelcomeemailtaskrun-method-directly","text":"result = SendWelcomeEmailTask.sync(user_id=some_user.id, custom_message=\"Testing sync call.\") print(result) # Output: {'status': 'success', 'user_id': ..., 'email': ...} ``` until(task_kwargs: dict, max_eta: datetime) - Schedule Randomly Before a Deadline: Schedules the task to run at a random time between now and the specified max_eta (maximum execution time). Useful for distributing load for non-time-critical tasks. It's not a load balancer, it's just a way to schedule tasks to run at different times. ```python from django.utils import timezone from datetime import timedelta","title":"This will run the SendWelcomeEmailTask.run() method directly"},{"location":"on_demand_tasks/#process-a-batch-of-optional-analytics-updates-sometime-in-the-next-hour","text":"max_execution_time = timezone.now() + timedelta(hours=1) class ProcessAnalyticsBatch(Task): def run(self, user_ids: list[int]): for user_id in user_ids: SendWelcomeEmailTask.until(task_kwargs={\"user_id\": user_id}, max_eta=max_execution_time) ```","title":"Process a batch of optional analytics updates sometime in the next hour"},{"location":"on_demand_tasks/#advanced-task-configuration","text":"You can fine-tune task behavior using several class attributes and methods on your Task subclass. These configurations often map to Google Cloud Tasks API features.","title":"Advanced Task Configuration"},{"location":"on_demand_tasks/#ensuring-uniqueness-only_once","text":"Set only_once = True on your task class if you want to prevent duplicate enqueues of tasks that are identical by class name for a given queue. Cloud Tasks will use a deterministic task name derived from your task's class name. Use Case: Imagine you have a task ProcessOrderTask that gets called when an order is submitted. If, due to a client-side retry or a race condition, your system tries to enqueue ProcessOrderTask for the same order multiple times in quick succession before the first one is picked up , only_once = True can help prevent multiple identical tasks for that order from being added to the queue initially. Caveat: This de-duplication is based on the task class name . It doesn't consider task arguments. If you need de-duplication based on specific arguments (e.g., \"only one ProcessOrderTask for order_id=123 \"), you'd need to implement that logic yourself, perhaps by checking for an existing task with a custom-generated name via the push() method, or within your run method. class ProcessOrderTask(Task): only_once = True # Prevents rapid, identical enqueues by class name def run(self, order_id: int, payment_details: dict): print(f\"Processing order {order_id}...\") # ... order processing logic ...","title":"Ensuring Uniqueness: only_once"},{"location":"on_demand_tasks/#enqueue-retry-policy","text":"These settings control retries if the the act of trying to send the task to Google Cloud Tasks fails, for example, due to a transient network issue or a temporary problem with the Cloud Tasks API. This is not about retrying your task if its run() method fails. enqueue_retry_exceptions: list[str | Type[Exception]] | None : A list of exception types (or their string paths like 'google.api_core.exceptions.ServiceUnavailable' ) that should trigger a retry. Defaults to an empty list (or what's set globally via DJANGO_CLOUD_TASKS_ENQUEUE_RETRY_EXCEPTIONS ). enqueue_retry_initial: float | None : Initial delay in seconds for the first retry. Defaults to global config. enqueue_retry_maximum: float | None : Maximum delay in seconds between retries. Defaults to global config. enqueue_retry_multiplier: float | None : Multiplier for increasing the delay between retries. Defaults to global config. enqueue_retry_deadline: float | None : Total time in seconds to keep retrying. Defaults to global config. from google.api_core.exceptions import ServiceUnavailable, InternalServerError class RobustEnqueueTask(Task): enqueue_retry_exceptions = [ServiceUnavailable, InternalServerError, \"requests.exceptions.ConnectionError\"] enqueue_retry_initial = 2.0 # Start with a 2-second delay enqueue_retry_maximum = 60.0 # Cap retries at 1 minute intervals enqueue_retry_multiplier = 2.5 enqueue_retry_deadline = 300.0 # Try for up to 5 minutes def run(self, data: dict): print(\"Task enqueued robustly and now running.\")","title":"Enqueue Retry Policy"},{"location":"on_demand_tasks/#using-custom-queue-names","text":"By default, tasks are sent to a queue named after your DJANGO_CLOUD_TASKS_APP_NAME (or just \"tasks\" if APP_NAME is not set). You can direct specific tasks to different queues by overriding the queue() class method. This is useful for prioritizing tasks or managing different workloads (e.g., short, quick tasks vs. long-running batch jobs). Make sure any custom queues you specify actually exist in your Google Cloud Tasks project. class HighPriorityNotification(Task): @classmethod def queue(cls) -> str: return \"critical-notifications-queue\" # Ensure this queue is configured in GCP with appropriate retries etc. def run(self, user_id: int, alert_message: str): # ... send an urgent alert ... print(f\"Sent high priority alert to user {user_id}\") class BatchDataProcessing(Task): @classmethod def queue(cls) -> str: return \"batch-jobs-queue\" # A queue for longer, less time-sensitive tasks def run(self, dataset_id: str): # ... process a large dataset ... print(f\"Processed batch data for {dataset_id}\")","title":"Using Custom Queue Names"},{"location":"on_demand_tasks/#suggesting-a-task-timeout","text":"Cloud Tasks need to know how long to wait for your task to complete before considering it timed out. You can suggest a timeout by overriding get_task_timeout() . The actual timeout is ultimately controlled by the queue configuration in GCP, but this provides a hint. from datetime import timedelta class ReportGenerationTask(Task): @classmethod def get_task_timeout(cls) -> timedelta | None: # This report can sometimes take a while return timedelta(minutes=15) # Suggest Cloud Tasks allow up to 15 minutes def run(self, report_params: dict): # ... logic to generate a potentially long report ... print(\"Report generation complete.\")","title":"Suggesting a Task Timeout"},{"location":"on_demand_tasks/#accessing-task-metadata-self_metadata","text":"Inside your task's run method, you can access information about the current execution attempt via self._metadata . This is an instance of TaskMetadata (or your custom class specified by DJANGO_CLOUD_TASKS_TASK_METADATA_CLASS ). Use Case: You might want to log the attempt number, or have slightly different behavior on the first attempt versus a retry (e.g., more aggressive external API calls on first try, then back off). class RetryAwareTask(Task): def run(self, api_call_details: dict): print(f\"--- Task Execution --- \") print(f\"Task ID: {self._metadata.task_id}\") print(f\"Queue: {self._metadata.queue_name}\") print(f\"Attempt Number (dispatch count + 1): {self._metadata.attempt_number}\") print(f\"Execution Number (non-5XX responses): {self._metadata.execution_number}\") print(f\"Scheduled ETA: {self._metadata.eta}\") if self._metadata.previous_failure: print(f\"This task previously failed with reason: {self._metadata.previous_failure}\") if self._metadata.is_cloud_scheduler: print(f\"This task was triggered by Cloud Scheduler job: {self._metadata.cloud_scheduler_job_name}\") # Example: Different logic for first attempt vs retries if self._metadata.first_attempt: print(\"First attempt: trying the primary API endpoint.\") # make_api_call(api_call_details, endpoint_type=\"primary\") else: print(\"Retry attempt: trying a fallback API endpoint or with reduced payload.\") # make_api_call(api_call_details, endpoint_type=\"fallback\") # Your main task logic here # ...","title":"Accessing Task Metadata (self._metadata)"},{"location":"on_demand_tasks/#using-push-for-full-control","text":"For the most granular control over task enqueuing, you can use the push() class method directly. asap() and later() are convenient wrappers around push() . This allows you to specify custom headers, a dynamically generated task name (useful for more specific de-duplication than only_once ), or override the queue dynamically. # Generate a unique task name for de-duplication based on arguments # This requires more careful management than `only_once = True` unique_part = hashlib.md5(f\"{user_id}-{item_id}\".encode()).hexdigest() deterministic_task_name = f\"ProcessItemTask-{unique_part}\" SendWelcomeEmailTask.push( task_kwargs={\"user_id\": 777, \"custom_message\": \"Pushed with full control!\"}, headers={\"X-Correlation-ID\": \"my-custom-trace-id\"}, # Custom headers for this specific push delay_in_seconds=60, # Delay by 1 minute queue=\"another-dynamic-queue\", # Can override queue here too task_name=deterministic_task_name, # Provide a specific task name )","title":"Using push() for Full Control"},{"location":"on_demand_tasks/#managing-and-debugging-tasks","text":"Django Cloud Tasks provides a couple of utility methods on your task classes for interacting with tasks already in a queue .","title":"Managing and Debugging Tasks"},{"location":"on_demand_tasks/#debugging-a-specific-task-debug","text":"If a task has failed in the cloud, or you want to re-run a specific execution locally with the exact same payload it had, you can use debug() . How it works: It fetches the task details (including its original payload) from Google Cloud Tasks using its task_id and then executes the run() method locally (synchronously) with that payload. It also populates self._metadata from the fetched task information. # In a Django shell or management command: # from myapp.tasks import SendWelcomeEmailTask # Assume you got this task ID from your GCP logs task_id_from_gcp = \"1234567890abcdef\" # This will fetch the task, deserialize its payload, and run it locally: SendWelcomeEmailTask.debug(task_id=task_id_from_gcp)","title":"Debugging a Specific Task (debug())"},{"location":"on_demand_tasks/#discarding-tasks-discard","text":"You can remove tasks from a Google Cloud Tasks queue. Discard a specific task by ID: python # SendWelcomeEmailTask.discard(task_id=\"projects/.../tasks/someSpecificId\") Discard all tasks of a certain type from a queue (use with caution!): python # This will list all tasks for SendWelcomeEmailTask in its default queue and delete them. # deleted_tasks_info = SendWelcomeEmailTask.discard() # print(f\"Deleted tasks: {deleted_tasks_info}\") Discard tasks of a certain type that have met a minimum retry count: Useful for clearing out tasks that seem to be failing persistently after a certain number of attempts. python # Delete all SendWelcomeEmailTask instances from the queue that have been dispatched (retried) at least 5 times. # SendWelcomeEmailTask.discard(min_retries=5)","title":"Discarding Tasks (discard())"},{"location":"on_demand_tasks/#handling-failed-tasks","text":"Any exception raised by your task's run() method is considered a failure. This will inform Google Cloud Tasks to retry the task, according to the policy you've set. If you want to skip retrying a task, you can raise DiscardTaskException from within your task. This covers the ins and outs of on-demand tasks. They form the foundation for many asynchronous operations in your Django application.","title":"Handling Failed Tasks"},{"location":"pubsub/","text":"Publishing & Subscribing (Google Cloud Pub/Sub) Django Cloud Tasks seamlessly integrates with Google Cloud Pub/Sub, enabling you to build powerful event-driven architectures. You can publish messages to Pub/Sub topics when something interesting happens in your application, and define subscriber tasks that react to these messages asynchronously. Publishing Messages Messages are published to specific \"topics.\" You can think of a topic as a named channel for a certain category of events (e.g., \"user-signups\", \"order-updates\"). There are two main base classes for creating publishers: PublisherTask : For publishing general-purpose dictionary-based messages. ModelPublisherTask : A specialized helper for easily publishing messages related to Django model instance events (e.g., when a model is created, updated, or deleted). 1. Basic Publisher: PublisherTask Inherit from PublisherTask to define a generic message publisher. The primary method to override is topic_name() . Example: Publishing User Action Events Let's say we want to publish an event whenever a critical user action occurs, like a password change or profile update. # In your app's tasks.py or a dedicated publishers.py file from django_cloud_tasks.tasks import PublisherTask class UserActionEventPublisher(PublisherTask): @classmethod def topic_name(cls) -> str: # This will be the base name for your topic. # The final name in GCP might be prefixed (see \"Topic Naming\" below). return \"user-actions\" # --- How to use it --- # In your views.py, after a user successfully changes their password: # user = request.user # event_payload = { # \"user_id\": user.id, # \"action_type\": \"password_changed\", # \"ip_address\": get_client_ip(request), # A helper function to get IP # \"timestamp\": timezone.now().isoformat() # } # UserActionEventPublisher.asap(message=event_payload, attributes={\"priority\": \"high\"}) # Or publish synchronously (e.g., for tests or if DJANGO_CLOUD_TASKS_EAGER = True): # UserActionEventPublisher.sync(message=event_payload, attributes={\"source\": \"test_suite\"}) How to run PublisherTask : * YourPublisher.asap(message: dict, attributes: dict[str, str] | None = None) : Enqueues the publishing action itself as an on-demand task (via Cloud Tasks) to publish the message to Pub/Sub. This makes the HTTP request that publishes the message asynchronous. * YourPublisher.sync(message: dict, attributes: dict[str, str] | None = None) : Directly publishes the message to Pub/Sub in the current process. 2. Model-Specific Publisher: ModelPublisherTask This class is incredibly useful when the event you want to publish is directly tied to a Django model instance (e.g., an Order was created, an Article was updated). Example: Publishing Order Creation Events # In your app's tasks.py or publishers.py # Assuming you have an Order model: models.py # class Order(models.Model): # order_id = models.UUIDField(primary_key=True, default=uuid.uuid4) # user = models.ForeignKey(User, on_delete=models.CASCADE) # total_amount = models.DecimalField(max_digits=10, decimal_places=2) # status = models.CharField(max_length=50, default=\"pending\") # created_at = models.DateTimeField(auto_now_add=True) from django.db import models # Your Django models from django_cloud_tasks.tasks import ModelPublisherTask class OrderCreatedEvent(ModelPublisherTask): @classmethod def build_message_content(cls, obj: models.Model, **kwargs) -> dict: # obj is an instance of your Django model (e.g., an Order instance) # kwargs can receive any extra arguments passed to asap(), sync(), etc. order = obj # Explicitly cast/type hint if needed return { \"order_id\": str(order.order_id), \"user_id\": order.user_id, \"total_amount\": float(order.total_amount), # Pub/Sub prefers basic JSON types \"status\": order.status, \"created_at_iso\": order.created_at.isoformat(), \"campaign_source\": kwargs.get(\"campaign_source\") # Example of using an extra kwarg } @classmethod def build_message_attributes(cls, obj: models.Model, **kwargs) -> dict[str, str]: order = obj return { \"event_type\": \"order_created\", \"customer_segment\": \"retail\", # Example attribute \"region\": kwargs.get(\"region\", \"unknown\") } # topic_name() by default uses the model's app_label and model_name (e.g., \"myapp-order\") # You can override it if needed (see \"Customizing Publishers\" below). # --- How to use it --- # After an order instance is created and saved: # new_order = Order.objects.create(user=request.user, total_amount=cart.total, ...) # Publish ASAP: # OrderCreatedEvent.asap(obj=new_order, campaign_source=\"spring_sale\", region=\"emea\") # Or, to ensure the message is sent ONLY if the current database transaction commits successfully: # from django.db import transaction # with transaction.atomic(): # new_order.save() # Or new_order.objects.create(...) # OrderCreatedEvent.sync_on_commit(obj=new_order, campaign_source=\"newsletter\") Key methods for ModelPublisherTask : build_message_content(cls, obj: Model, **kwargs) -> dict (Required): You implement this to transform your model instance ( obj ) and any extra kwargs into the main JSON payload of the Pub/Sub message. build_message_attributes(cls, obj: Model, **kwargs) -> dict[str, str] (Required): You implement this to create a dictionary of string-to-string attributes for the Pub/Sub message. Attributes are useful for filtering messages on the subscriber side without needing to parse the full JSON payload. sync_on_commit(obj: Model, **kwargs) : A very handy method that delays the actual publishing until the current database transaction is successfully committed. This prevents sending messages for data that might be rolled back. Topic Naming Convention Default topic_name() for PublisherTask : Uses the class name (e.g., UserActionEventPublisher becomes topic base name UserActionEventPublisher ). Default topic_name() for ModelPublisherTask : Uses app_label-model_name (e.g., if Order is in sales app, it becomes sales-order ). Global Prefixing: If DJANGO_CLOUD_TASKS_APP_NAME is set in your Django settings (e.g., to \"my-ecom-service\" ), this name, along with the DJANGO_CLOUD_TASKS_DELIMITER (default \"--\" ), will be prepended to the base topic name. So, UserActionEventPublisher could become my-ecom-service--UserActionEventPublisher in GCP. This prefixing helps organize topics in GCP, especially if multiple services share a project. Ensuring Topics Exist ( set_up ) PublisherTask (and by extension ModelPublisherTask ) has a set_up() class method. Calling YourPublisherTask.set_up() will attempt to create the Pub/Sub topic in GCP if it doesn't already exist. # You might call this in an AppConfig.ready() or a custom management command # UserActionEventPublisher.set_up() # OrderCreatedEvent.set_up() # For ModelPublisherTask, it uses the default topic name based on model This does not set up IAM permissions for publishing; your service account running the Django app needs pubsub.topics.publish permission on the topic or project. Subscribing to Messages ( SubscriberTask ) To process messages published to a topic, you define a SubscriberTask . This task will be triggered via an HTTP push request from Google Cloud Pub/Sub to a dedicated endpoint in your Django application when a new message arrives on the subscribed topic. Example: Processing User Action Events and Order Notifications # In your app's tasks.py (or a dedicated subscribers.py file) from django_cloud_tasks.tasks import SubscriberTask # from myapp.services import fraud_detection_service, notification_service class UserActionAuditor(SubscriberTask): @classmethod def topic_name(cls) -> str: # This MUST match the topic name used by UserActionEventPublisher return \"user-actions\" # The run method receives the deserialized message content and attributes def run(self, content: dict, attributes: dict[str, str] | None = None): print(f\"Auditing user action: {content.get('action_type')} for user {content.get('user_id')}\") print(f\" Attributes: {attributes}\") # if content.get('action_type') == 'password_changed': # fraud_detection_service.check_suspicious_login_after_password_change(content) return {\"status\": \"action_audited\", \"user_id\": content.get('user_id')} class OrderNotificationHandler(SubscriberTask): @classmethod def topic_name(cls) -> str: # This MUST match the topic from OrderCreatedEvent. For an Order model in 'sales' app: return \"sales-order\" # Or your custom topic name if overridden in ModelPublisherTask def run(self, content: dict, attributes: dict[str, str] | None = None): print(f\"New order received for processing: {content.get('order_id')}\") print(f\" Event Type (from attribute): {attributes.get('event_type')}\") # notification_service.send_order_confirmation_email(content.get('user_id'), content) # inventory_service.reserve_stock(content.get('order_id'), ...) return {\"status\": \"order_processed\", \"order_id\": content.get('order_id')} Key elements for SubscriberTask : topic_name(cls) -> str (Required): Specifies which Pub/Sub topic this task subscribes to. This name needs to match the base name of the publisher's topic (before any global APP_NAME prefixing). run(content: dict, attributes: dict[str, str] | None = None) : Your core logic to handle the incoming message. content is the deserialized JSON payload, and attributes are the string key-value pairs sent with the Pub/Sub message. Subscription Naming Convention Default subscription_name() : Similar to topics, the subscription name is derived from DJANGO_CLOUD_TASKS_APP_NAME (if set), the DJANGO_CLOUD_TASKS_DELIMITER , and the SubscriberTask class name (e.g., my-ecom-service--UserActionAuditor ). This name is used for the actual Pub/Sub Subscription resource created in GCP. Setting Up and Deploying Subscriptions Defining the SubscriberTask class in Python doesn't automatically create the subscription in Google Cloud Pub/Sub. You need to run a management command: python manage.py initialize_subscribers What this command does: Scans your project for all SubscriberTask classes. For each task, it calls its set_up() class method. The default set_up() method in SubscriberTask will: Attempt to create a Pub/Sub topic (using the subscriber's topic_name() ) if it doesn't already exist. This is a safety measure; ideally, publishers manage their topics. Create or update a Pub/Sub subscription (using subscription_name() ) to that topic. Configure the subscription to PUSH messages via HTTP to a Django endpoint specific to that SubscriberTask (derived from subscription_url() ). Enable OIDC authentication by default for these push requests (see _use_oidc_auth customization). Apply other subscription settings like retry policies, dead-letter topics, and filters if customized on the task class. The command will output [+] , [~] , [-] for added, updated, or (less commonly) deleted subscriptions. When to run initialize_subscribers ? Run this as part of your deployment process, especially when you add new SubscriberTask s or change their subscription configurations (like topic_name , filter , retry policies, etc.). Customizing Publishers Custom Topic Names for Publishers For both PublisherTask and ModelPublisherTask , you can override topic_name(cls, ...) for more control. class LegacySystemEventPublisher(PublisherTask): @classmethod def topic_name(cls) -> str: # Overrides the default naming based on class name return \"legacy-integration-bus\" # For ModelPublisherTask, topic_name can also use the object class ProductUpdateToSpecificChannel(ModelPublisherTask): @classmethod def topic_name(cls, obj: models.Model, **kwargs) -> str: product = obj # Example: route product updates to different topics based on category if product.category == \"electronics\": return \"product-updates-electronics\" return \"product-updates-general\" # ... build_message_content and build_message_attributes ... Remember that if DJANGO_CLOUD_TASKS_APP_NAME is set, it will still be prefixed unless your override includes it or is absolute. Customizing Subscribers SubscriberTask offers several attributes and methods for fine-tuning the GCP Pub/Sub subscription. Custom Subscription Name ( subscription_name ) While default naming is usually fine, you can override subscription_name() if needed, similar to schedule_name for periodic tasks. Custom Subscription URL ( subscription_url ) This is rarely needed, as the default URL points to the correct handler in django-cloud-tasks . Overriding this means you're pointing Pub/Sub to a custom endpoint you've built. OIDC Authentication for Push Endpoint ( _use_oidc_auth ) Class attribute _use_oidc_auth: bool = True . Controls if the Pub/Sub push subscription expects Google to send an OIDC token for authentication. Generally, keep this True if your Django app runs on a service like Cloud Run that can validate these tokens. Subscription Retry Policy (Message Acknowledgement Deadline & Backoff) These settings on your SubscriberTask class map to the Pub/Sub subscription's message delivery retry configuration. They define how Pub/Sub handles messages if your endpoint doesn't acknowledge them (e.g., returns an error or times out). max_retries: int | None = UNSET : Maximum delivery attempts before sending to a dead-letter topic (if configured). Defaults to global DJANGO_CLOUD_TASKS_SUBSCRIBER_MAX_RETRIES or GCP default. min_backoff: int | None = UNSET : Minimum delay (in seconds) Pub/Sub waits before redelivering an unacknowledged message. Defaults to global DJANGO_CLOUD_TASKS_SUBSCRIBER_MIN_BACKOFF or GCP default (typically 10s). max_backoff: int | None = UNSET : Maximum delay (in seconds) for redelivery. Defaults to global DJANGO_CLOUD_TASKS_SUBSCRIBER_MAX_BACKOFF or GCP default (typically 600s). class TimeSensitiveAlertSubscriber(SubscriberTask): topic_name = \"critical-alerts\" min_backoff = 5 # Retry quickly for these alerts, minimum 5 seconds max_backoff = 60 # But don't wait too long, max 1 minute max_retries = 3 # Only try 3 times before considering it failed (e.g., for dead-lettering) def run(self, content: dict, attributes: dict[str, str] | None = None): print(f\"Processing time-sensitive alert: {content}\") # ... alert processing ... Dead Letter Topics (DLT/DLQ) If a message consistently fails processing after configured retries, Pub/Sub can forward it to a Dead Letter Topic (DLT), effectively a Dead Letter Queue (DLQ). dead_letter_topic_name(cls) -> str | None : Override to return the base name of the DLT. If None (default), no DLT is used for this subscriber. dead_letter_subscription_name(cls) -> str : Name for the subscription to the DLT (often just the DLT name). The initialize_subscribers command will attempt to set up the DLT and necessary permissions if you configure this. You'll need a separate process or subscriber to monitor and handle messages in the DLT. class PaymentProcessingSubscriber(SubscriberTask): topic_name = \"payment-requests\" max_retries = 5 # After 5 failed attempts, send to DLT @classmethod def dead_letter_topic_name(cls) -> str | None: return \"payment-requests-failed\" # Base name for the DLT def run(self, content: dict, attributes: dict[str, str] | None = None): # ... process payment ... # if permanent_failure_condition(content): # raise DiscardTaskException() # To prevent retries and avoid DLT for known bad messages pass Message Filtering ( subscription_filter ) Pub/Sub allows subscriptions to specify a filter, so the subscription only receives messages whose attributes match the filter. This can reduce the number of messages your subscriber task needs to process. subscription_filter(cls) -> str | None : Return a filter string based on Pub/Sub filter syntax . class HighPriorityOrderSubscriber(SubscriberTask): topic_name = \"sales-order\" # Subscribes to the same topic as OrderNotificationHandler @classmethod def subscription_filter(cls) -> str | None: # Only receive messages where the 'priority' attribute is 'high' return 'attributes.priority = \"high\"' def run(self, content: dict, attributes: dict[str, str] | None = None): print(f\"Processing HIGH PRIORITY order: {content.get('order_id')}\") # ... specialized high-priority handling ... Custom Message Parsing ( message_parser ) message_parser(cls) -> Callable : A callable used to parse the raw message body received from Pub/Sub. Defaults to json.loads . You'd only override this if your publishers are sending non-JSON messages (e.g., raw text, protobuf), which is less common when using this library's PublisherTask which serializes to JSON. This event-driven model using Pub/Sub provides a robust and scalable way to build decoupled applications where services can communicate and react to events without direct dependencies.","title":"Pub/Sub"},{"location":"pubsub/#publishing-subscribing-google-cloud-pubsub","text":"Django Cloud Tasks seamlessly integrates with Google Cloud Pub/Sub, enabling you to build powerful event-driven architectures. You can publish messages to Pub/Sub topics when something interesting happens in your application, and define subscriber tasks that react to these messages asynchronously.","title":"Publishing &amp; Subscribing (Google Cloud Pub/Sub)"},{"location":"pubsub/#publishing-messages","text":"Messages are published to specific \"topics.\" You can think of a topic as a named channel for a certain category of events (e.g., \"user-signups\", \"order-updates\"). There are two main base classes for creating publishers: PublisherTask : For publishing general-purpose dictionary-based messages. ModelPublisherTask : A specialized helper for easily publishing messages related to Django model instance events (e.g., when a model is created, updated, or deleted).","title":"Publishing Messages"},{"location":"pubsub/#1-basic-publisher-publishertask","text":"Inherit from PublisherTask to define a generic message publisher. The primary method to override is topic_name() . Example: Publishing User Action Events Let's say we want to publish an event whenever a critical user action occurs, like a password change or profile update. # In your app's tasks.py or a dedicated publishers.py file from django_cloud_tasks.tasks import PublisherTask class UserActionEventPublisher(PublisherTask): @classmethod def topic_name(cls) -> str: # This will be the base name for your topic. # The final name in GCP might be prefixed (see \"Topic Naming\" below). return \"user-actions\" # --- How to use it --- # In your views.py, after a user successfully changes their password: # user = request.user # event_payload = { # \"user_id\": user.id, # \"action_type\": \"password_changed\", # \"ip_address\": get_client_ip(request), # A helper function to get IP # \"timestamp\": timezone.now().isoformat() # } # UserActionEventPublisher.asap(message=event_payload, attributes={\"priority\": \"high\"}) # Or publish synchronously (e.g., for tests or if DJANGO_CLOUD_TASKS_EAGER = True): # UserActionEventPublisher.sync(message=event_payload, attributes={\"source\": \"test_suite\"}) How to run PublisherTask : * YourPublisher.asap(message: dict, attributes: dict[str, str] | None = None) : Enqueues the publishing action itself as an on-demand task (via Cloud Tasks) to publish the message to Pub/Sub. This makes the HTTP request that publishes the message asynchronous. * YourPublisher.sync(message: dict, attributes: dict[str, str] | None = None) : Directly publishes the message to Pub/Sub in the current process.","title":"1. Basic Publisher: PublisherTask"},{"location":"pubsub/#2-model-specific-publisher-modelpublishertask","text":"This class is incredibly useful when the event you want to publish is directly tied to a Django model instance (e.g., an Order was created, an Article was updated). Example: Publishing Order Creation Events # In your app's tasks.py or publishers.py # Assuming you have an Order model: models.py # class Order(models.Model): # order_id = models.UUIDField(primary_key=True, default=uuid.uuid4) # user = models.ForeignKey(User, on_delete=models.CASCADE) # total_amount = models.DecimalField(max_digits=10, decimal_places=2) # status = models.CharField(max_length=50, default=\"pending\") # created_at = models.DateTimeField(auto_now_add=True) from django.db import models # Your Django models from django_cloud_tasks.tasks import ModelPublisherTask class OrderCreatedEvent(ModelPublisherTask): @classmethod def build_message_content(cls, obj: models.Model, **kwargs) -> dict: # obj is an instance of your Django model (e.g., an Order instance) # kwargs can receive any extra arguments passed to asap(), sync(), etc. order = obj # Explicitly cast/type hint if needed return { \"order_id\": str(order.order_id), \"user_id\": order.user_id, \"total_amount\": float(order.total_amount), # Pub/Sub prefers basic JSON types \"status\": order.status, \"created_at_iso\": order.created_at.isoformat(), \"campaign_source\": kwargs.get(\"campaign_source\") # Example of using an extra kwarg } @classmethod def build_message_attributes(cls, obj: models.Model, **kwargs) -> dict[str, str]: order = obj return { \"event_type\": \"order_created\", \"customer_segment\": \"retail\", # Example attribute \"region\": kwargs.get(\"region\", \"unknown\") } # topic_name() by default uses the model's app_label and model_name (e.g., \"myapp-order\") # You can override it if needed (see \"Customizing Publishers\" below). # --- How to use it --- # After an order instance is created and saved: # new_order = Order.objects.create(user=request.user, total_amount=cart.total, ...) # Publish ASAP: # OrderCreatedEvent.asap(obj=new_order, campaign_source=\"spring_sale\", region=\"emea\") # Or, to ensure the message is sent ONLY if the current database transaction commits successfully: # from django.db import transaction # with transaction.atomic(): # new_order.save() # Or new_order.objects.create(...) # OrderCreatedEvent.sync_on_commit(obj=new_order, campaign_source=\"newsletter\") Key methods for ModelPublisherTask : build_message_content(cls, obj: Model, **kwargs) -> dict (Required): You implement this to transform your model instance ( obj ) and any extra kwargs into the main JSON payload of the Pub/Sub message. build_message_attributes(cls, obj: Model, **kwargs) -> dict[str, str] (Required): You implement this to create a dictionary of string-to-string attributes for the Pub/Sub message. Attributes are useful for filtering messages on the subscriber side without needing to parse the full JSON payload. sync_on_commit(obj: Model, **kwargs) : A very handy method that delays the actual publishing until the current database transaction is successfully committed. This prevents sending messages for data that might be rolled back.","title":"2. Model-Specific Publisher: ModelPublisherTask"},{"location":"pubsub/#topic-naming-convention","text":"Default topic_name() for PublisherTask : Uses the class name (e.g., UserActionEventPublisher becomes topic base name UserActionEventPublisher ). Default topic_name() for ModelPublisherTask : Uses app_label-model_name (e.g., if Order is in sales app, it becomes sales-order ). Global Prefixing: If DJANGO_CLOUD_TASKS_APP_NAME is set in your Django settings (e.g., to \"my-ecom-service\" ), this name, along with the DJANGO_CLOUD_TASKS_DELIMITER (default \"--\" ), will be prepended to the base topic name. So, UserActionEventPublisher could become my-ecom-service--UserActionEventPublisher in GCP. This prefixing helps organize topics in GCP, especially if multiple services share a project.","title":"Topic Naming Convention"},{"location":"pubsub/#ensuring-topics-exist-set_up","text":"PublisherTask (and by extension ModelPublisherTask ) has a set_up() class method. Calling YourPublisherTask.set_up() will attempt to create the Pub/Sub topic in GCP if it doesn't already exist. # You might call this in an AppConfig.ready() or a custom management command # UserActionEventPublisher.set_up() # OrderCreatedEvent.set_up() # For ModelPublisherTask, it uses the default topic name based on model This does not set up IAM permissions for publishing; your service account running the Django app needs pubsub.topics.publish permission on the topic or project.","title":"Ensuring Topics Exist (set_up)"},{"location":"pubsub/#subscribing-to-messages-subscribertask","text":"To process messages published to a topic, you define a SubscriberTask . This task will be triggered via an HTTP push request from Google Cloud Pub/Sub to a dedicated endpoint in your Django application when a new message arrives on the subscribed topic. Example: Processing User Action Events and Order Notifications # In your app's tasks.py (or a dedicated subscribers.py file) from django_cloud_tasks.tasks import SubscriberTask # from myapp.services import fraud_detection_service, notification_service class UserActionAuditor(SubscriberTask): @classmethod def topic_name(cls) -> str: # This MUST match the topic name used by UserActionEventPublisher return \"user-actions\" # The run method receives the deserialized message content and attributes def run(self, content: dict, attributes: dict[str, str] | None = None): print(f\"Auditing user action: {content.get('action_type')} for user {content.get('user_id')}\") print(f\" Attributes: {attributes}\") # if content.get('action_type') == 'password_changed': # fraud_detection_service.check_suspicious_login_after_password_change(content) return {\"status\": \"action_audited\", \"user_id\": content.get('user_id')} class OrderNotificationHandler(SubscriberTask): @classmethod def topic_name(cls) -> str: # This MUST match the topic from OrderCreatedEvent. For an Order model in 'sales' app: return \"sales-order\" # Or your custom topic name if overridden in ModelPublisherTask def run(self, content: dict, attributes: dict[str, str] | None = None): print(f\"New order received for processing: {content.get('order_id')}\") print(f\" Event Type (from attribute): {attributes.get('event_type')}\") # notification_service.send_order_confirmation_email(content.get('user_id'), content) # inventory_service.reserve_stock(content.get('order_id'), ...) return {\"status\": \"order_processed\", \"order_id\": content.get('order_id')} Key elements for SubscriberTask : topic_name(cls) -> str (Required): Specifies which Pub/Sub topic this task subscribes to. This name needs to match the base name of the publisher's topic (before any global APP_NAME prefixing). run(content: dict, attributes: dict[str, str] | None = None) : Your core logic to handle the incoming message. content is the deserialized JSON payload, and attributes are the string key-value pairs sent with the Pub/Sub message.","title":"Subscribing to Messages (SubscriberTask)"},{"location":"pubsub/#subscription-naming-convention","text":"Default subscription_name() : Similar to topics, the subscription name is derived from DJANGO_CLOUD_TASKS_APP_NAME (if set), the DJANGO_CLOUD_TASKS_DELIMITER , and the SubscriberTask class name (e.g., my-ecom-service--UserActionAuditor ). This name is used for the actual Pub/Sub Subscription resource created in GCP.","title":"Subscription Naming Convention"},{"location":"pubsub/#setting-up-and-deploying-subscriptions","text":"Defining the SubscriberTask class in Python doesn't automatically create the subscription in Google Cloud Pub/Sub. You need to run a management command: python manage.py initialize_subscribers What this command does: Scans your project for all SubscriberTask classes. For each task, it calls its set_up() class method. The default set_up() method in SubscriberTask will: Attempt to create a Pub/Sub topic (using the subscriber's topic_name() ) if it doesn't already exist. This is a safety measure; ideally, publishers manage their topics. Create or update a Pub/Sub subscription (using subscription_name() ) to that topic. Configure the subscription to PUSH messages via HTTP to a Django endpoint specific to that SubscriberTask (derived from subscription_url() ). Enable OIDC authentication by default for these push requests (see _use_oidc_auth customization). Apply other subscription settings like retry policies, dead-letter topics, and filters if customized on the task class. The command will output [+] , [~] , [-] for added, updated, or (less commonly) deleted subscriptions. When to run initialize_subscribers ? Run this as part of your deployment process, especially when you add new SubscriberTask s or change their subscription configurations (like topic_name , filter , retry policies, etc.).","title":"Setting Up and Deploying Subscriptions"},{"location":"pubsub/#customizing-publishers","text":"","title":"Customizing Publishers"},{"location":"pubsub/#custom-topic-names-for-publishers","text":"For both PublisherTask and ModelPublisherTask , you can override topic_name(cls, ...) for more control. class LegacySystemEventPublisher(PublisherTask): @classmethod def topic_name(cls) -> str: # Overrides the default naming based on class name return \"legacy-integration-bus\" # For ModelPublisherTask, topic_name can also use the object class ProductUpdateToSpecificChannel(ModelPublisherTask): @classmethod def topic_name(cls, obj: models.Model, **kwargs) -> str: product = obj # Example: route product updates to different topics based on category if product.category == \"electronics\": return \"product-updates-electronics\" return \"product-updates-general\" # ... build_message_content and build_message_attributes ... Remember that if DJANGO_CLOUD_TASKS_APP_NAME is set, it will still be prefixed unless your override includes it or is absolute.","title":"Custom Topic Names for Publishers"},{"location":"pubsub/#customizing-subscribers","text":"SubscriberTask offers several attributes and methods for fine-tuning the GCP Pub/Sub subscription.","title":"Customizing Subscribers"},{"location":"pubsub/#custom-subscription-name-subscription_name","text":"While default naming is usually fine, you can override subscription_name() if needed, similar to schedule_name for periodic tasks.","title":"Custom Subscription Name (subscription_name)"},{"location":"pubsub/#custom-subscription-url-subscription_url","text":"This is rarely needed, as the default URL points to the correct handler in django-cloud-tasks . Overriding this means you're pointing Pub/Sub to a custom endpoint you've built.","title":"Custom Subscription URL (subscription_url)"},{"location":"pubsub/#oidc-authentication-for-push-endpoint-_use_oidc_auth","text":"Class attribute _use_oidc_auth: bool = True . Controls if the Pub/Sub push subscription expects Google to send an OIDC token for authentication. Generally, keep this True if your Django app runs on a service like Cloud Run that can validate these tokens.","title":"OIDC Authentication for Push Endpoint (_use_oidc_auth)"},{"location":"pubsub/#subscription-retry-policy-message-acknowledgement-deadline-backoff","text":"These settings on your SubscriberTask class map to the Pub/Sub subscription's message delivery retry configuration. They define how Pub/Sub handles messages if your endpoint doesn't acknowledge them (e.g., returns an error or times out). max_retries: int | None = UNSET : Maximum delivery attempts before sending to a dead-letter topic (if configured). Defaults to global DJANGO_CLOUD_TASKS_SUBSCRIBER_MAX_RETRIES or GCP default. min_backoff: int | None = UNSET : Minimum delay (in seconds) Pub/Sub waits before redelivering an unacknowledged message. Defaults to global DJANGO_CLOUD_TASKS_SUBSCRIBER_MIN_BACKOFF or GCP default (typically 10s). max_backoff: int | None = UNSET : Maximum delay (in seconds) for redelivery. Defaults to global DJANGO_CLOUD_TASKS_SUBSCRIBER_MAX_BACKOFF or GCP default (typically 600s). class TimeSensitiveAlertSubscriber(SubscriberTask): topic_name = \"critical-alerts\" min_backoff = 5 # Retry quickly for these alerts, minimum 5 seconds max_backoff = 60 # But don't wait too long, max 1 minute max_retries = 3 # Only try 3 times before considering it failed (e.g., for dead-lettering) def run(self, content: dict, attributes: dict[str, str] | None = None): print(f\"Processing time-sensitive alert: {content}\") # ... alert processing ...","title":"Subscription Retry Policy (Message Acknowledgement Deadline &amp; Backoff)"},{"location":"pubsub/#dead-letter-topics-dltdlq","text":"If a message consistently fails processing after configured retries, Pub/Sub can forward it to a Dead Letter Topic (DLT), effectively a Dead Letter Queue (DLQ). dead_letter_topic_name(cls) -> str | None : Override to return the base name of the DLT. If None (default), no DLT is used for this subscriber. dead_letter_subscription_name(cls) -> str : Name for the subscription to the DLT (often just the DLT name). The initialize_subscribers command will attempt to set up the DLT and necessary permissions if you configure this. You'll need a separate process or subscriber to monitor and handle messages in the DLT. class PaymentProcessingSubscriber(SubscriberTask): topic_name = \"payment-requests\" max_retries = 5 # After 5 failed attempts, send to DLT @classmethod def dead_letter_topic_name(cls) -> str | None: return \"payment-requests-failed\" # Base name for the DLT def run(self, content: dict, attributes: dict[str, str] | None = None): # ... process payment ... # if permanent_failure_condition(content): # raise DiscardTaskException() # To prevent retries and avoid DLT for known bad messages pass","title":"Dead Letter Topics (DLT/DLQ)"},{"location":"pubsub/#message-filtering-subscription_filter","text":"Pub/Sub allows subscriptions to specify a filter, so the subscription only receives messages whose attributes match the filter. This can reduce the number of messages your subscriber task needs to process. subscription_filter(cls) -> str | None : Return a filter string based on Pub/Sub filter syntax . class HighPriorityOrderSubscriber(SubscriberTask): topic_name = \"sales-order\" # Subscribes to the same topic as OrderNotificationHandler @classmethod def subscription_filter(cls) -> str | None: # Only receive messages where the 'priority' attribute is 'high' return 'attributes.priority = \"high\"' def run(self, content: dict, attributes: dict[str, str] | None = None): print(f\"Processing HIGH PRIORITY order: {content.get('order_id')}\") # ... specialized high-priority handling ...","title":"Message Filtering (subscription_filter)"},{"location":"pubsub/#custom-message-parsing-message_parser","text":"message_parser(cls) -> Callable : A callable used to parse the raw message body received from Pub/Sub. Defaults to json.loads . You'd only override this if your publishers are sending non-JSON messages (e.g., raw text, protobuf), which is less common when using this library's PublisherTask which serializes to JSON. This event-driven model using Pub/Sub provides a robust and scalable way to build decoupled applications where services can communicate and react to events without direct dependencies.","title":"Custom Message Parsing (message_parser)"},{"location":"scheduled_tasks/","text":"Scheduled Tasks (Powered by Google Cloud Scheduler) Scheduled tasks, often known as \"cron jobs,\" are tasks that run automatically on a repeating schedule (e.g., every hour, once a day at 2 AM, every Monday morning). Django Cloud Tasks leverages Google Cloud Scheduler to manage and execute these periodic jobs. Your Django application defines what needs to run, and Cloud Scheduler takes care of triggering it at the right time via an HTTP request. Defining a Scheduled Task You define a scheduled task by inheriting from django_cloud_tasks.tasks.PeriodicTask . The most crucial part is setting the run_every class attribute to a cron expression that defines the schedule. Example: Daily Digest and Hourly Cleanup # In your app's tasks.py (e.g., reports/tasks.py or core/tasks.py) from django_cloud_tasks.tasks import PeriodicTask from django.utils import timezone # from myapp.models import UserActivity, TemporaryFile class GenerateDailyUserActivityDigest(PeriodicTask): # Cron expression: \"At 00:00 on every day-of-week from Monday through Friday.\" # (Assumes server/Cloud Scheduler timezone is UTC by default) run_every = \"0 0 * * 1-5\" def run(self, **kwargs): # kwargs will receive payload from Cloud Scheduler if any is set print(f\"Generating daily user activity digest for {timezone.now().date()}...\") # yesterday = timezone.now().date() - timezone.timedelta(days=1) # activities = UserActivity.objects.filter(timestamp__date=yesterday) # generate_report(activities) # send_report_to_admins() return {\"status\": \"digest_generated\", \"date\": str(timezone.now().date())} class HourlyTemporaryFileCleanup(PeriodicTask): run_every = \"@hourly\" # Cloud Scheduler shorthand for \"0 * * * *\" def run(self, older_than_hours: int = 2, **kwargs): print(f\"Cleaning up temporary files older than {older_than_hours} hours...\") # cutoff_time = timezone.now() - timezone.timedelta(hours=older_than_hours) # TemporaryFile.objects.filter(created_at__lt=cutoff_time).delete() return {\"status\": \"cleanup_done\", \"older_than_hours\": older_than_hours} Key Attributes for PeriodicTask : run_every: str (Required): Defines the schedule using the standard cron format . Cloud Scheduler also supports shorthands like @daily , @hourly , @weekly , @monthly , @yearly . run(**kwargs) : The method containing your task's logic. It's the same as for on-demand tasks. Any JSON payload you define in Cloud Scheduler (or through customization hooks) will be passed as kwargs . Deploying Scheduled Tasks Defining the Python class is just the first step. You need to inform Google Cloud Scheduler about your tasks. This is done using a Django management command: python manage.py schedule_tasks What this command does: It scans your Django project for all classes inheriting from PeriodicTask (ensure these modules are imported by Django at startup). It compares these definitions against the jobs currently configured in Google Cloud Scheduler for your DJANGO_CLOUD_TASKS_APP_NAME . It then synchronizes the state: [+] Added : New jobs are created in Cloud Scheduler for tasks found in your code but not yet scheduled. [~] Updated : Existing jobs in Cloud Scheduler are updated if their definition (e.g., cron schedule run_every , target URL, payload) has changed in your code. [-] Deleted : Jobs in Cloud Scheduler are deleted if their corresponding PeriodicTask class has been removed from your code. When to run schedule_tasks ? You should run this command as part of your deployment process, any time you add, remove, or modify the definition (especially run_every or other schedule-related overrides) of a PeriodicTask . Customizing Scheduled Task Behavior You can override several class methods on your PeriodicTask subclass to fine-tune how it interacts with Google Cloud Scheduler. Custom Job Naming ( schedule_name ) By default, the Cloud Scheduler job name is constructed using your DJANGO_CLOUD_TASKS_APP_NAME , the DJANGO_CLOUD_TASKS_DELIMITER , and the task's class name (e.g., my-app--GenerateDailyUserActivityDigest ). You can customize this for better organization or to adhere to specific naming conventions. class MonthlyBillingRun(PeriodicTask): run_every = \"0 3 1 * *\" # At 03:00 on day-of-month 1. @classmethod def schedule_name(cls) -> str: # Example: \"billing-service-MonthlyBillingRun-prod\" app_name = get_config(\"app_name\") # Fetches DJANGO_CLOUD_TASKS_APP_NAME env = \"prod\" if not settings.DEBUG else \"dev\" return f\"{app_name}-{cls.name()}-{env}\" # cls.name() is the class name def run(self, **kwargs): print(\"Starting monthly billing run...\") # ... billing logic ... Custom HTTP Headers for Scheduler Requests ( schedule_headers ) You can inject custom HTTP headers into the requests that Cloud Scheduler makes to your task endpoint. This can be useful for passing specific tokens, versioning information, or routing hints that your Django application or infrastructure might use. class DataSyncWithExternalService(PeriodicTask): run_every = \"0 */6 * * *\" # Every 6 hours @classmethod def schedule_headers(cls) -> dict: # This token might be used by your endpoint's authentication/authorization layer return {\"X-Scheduler-Auth-Token\": \"my-secret-scheduler-token\", \"X-Job-Type\": cls.name()} def run(self, **kwargs): # Your Django view or middleware can inspect request.META for these headers # e.g., request.META.get('HTTP_X_SCHEDULER_AUTH_TOKEN') print(\"Running data sync, triggered with custom headers.\") OIDC Authentication for Scheduler Requests ( schedule_use_oidc ) Determines if OIDC (OpenID Connect) authentication should be used for the HTTP request from Cloud Scheduler to your application. This is the recommended secure way for Google services to call your services. Default: True . If True , Cloud Scheduler will send a Google-signed OIDC token in the Authorization header. Your application endpoint (usually a Cloud Run service or similar) should be configured to validate this token. If False , no OIDC token is sent by Scheduler. You would rely on other means of securing your endpoint (e.g., network controls, custom schedule_headers with a pre-shared key, though OIDC is preferred). class PublicDataFetcherTask(PeriodicTask): run_every = \"@daily\" @classmethod def schedule_use_oidc(cls) -> bool: # If this task calls a public, non-OIDC protected part of your app, or if auth is handled differently return False # Set to True if your endpoint expects a Google OIDC token def run(self, **kwargs): print(\"Fetching public data...\") Scheduler-Level Retry Configuration ( schedule_retries ) Specifies the number of times Google Cloud Scheduler should retry invoking your task endpoint if the HTTP request fails (e.g., your endpoint returns a 5xx error or doesn't respond). Default: 0 (no retries at the Cloud Scheduler level). This is distinct from any retry logic within your task's run() method or Cloud Tasks queue retries (if the scheduled job itself pushes to a Cloud Tasks queue). class OccasionallyFailingExternalCheck(PeriodicTask): run_every = \"*/30 * * * *\" # Every 30 minutes @classmethod def schedule_retries(cls) -> int: # If the endpoint is down, ask Cloud Scheduler to retry up to 3 times return 3 def run(self, **kwargs): print(\"Performing an external check that might sometimes fail...\") # ... call external service ... Custom Payload for Scheduled Tasks While PeriodicTask.schedule() itself takes **kwargs which are serialized into the payload for Cloud Scheduler, you typically set these arguments directly in the run method's signature with defaults if they are static for the schedule. If you need to dynamically build a more complex payload during the schedule_tasks command or want to override what arguments are sent, you would need to customize the schedule method itself or ensure your run method pulls configuration dynamically. The default arguments sent in the payload by schedule() are simply the **kwargs passed to it. If no kwargs are passed (common for periodic tasks where the logic is self-contained or configured via settings ), the payload will be an empty JSON object {} . How It Works Under the Hood When PeriodicTask.schedule() is invoked (typically by the schedule_tasks management command), it communicates with Google Cloud Scheduler to create or update a job. This job is configured to: Trigger based on the run_every cron schedule. Make an HTTP POST request to the URL generated by YourTaskClass.url() (this defaults to the standard task execution endpoint in django_cloud_tasks.urls ). Include a JSON payload (by default, an empty dictionary {} unless kwargs were passed to schedule or the task sends a default payload). Use OIDC authentication and custom headers as configured. Essentially, Cloud Scheduler acts as a timed trigger that invokes your PeriodicTask as if it were an on-demand task call arriving at your application's endpoint. Eager Mode Behavior ( DJANGO_CLOUD_TASKS_EAGER = True ) If DJANGO_CLOUD_TASKS_EAGER is true, running python manage.py schedule_tasks will not actually interact with Google Cloud Scheduler. Instead, for any tasks that would have been added or updated, their run() method will be executed synchronously, locally, one time. This is primarily for testing the task logic itself, not the scheduling mechanism. And that's the rundown on keeping your tasks running like clockwork with scheduled tasks!","title":"Scheduled Tasks"},{"location":"scheduled_tasks/#scheduled-tasks-powered-by-google-cloud-scheduler","text":"Scheduled tasks, often known as \"cron jobs,\" are tasks that run automatically on a repeating schedule (e.g., every hour, once a day at 2 AM, every Monday morning). Django Cloud Tasks leverages Google Cloud Scheduler to manage and execute these periodic jobs. Your Django application defines what needs to run, and Cloud Scheduler takes care of triggering it at the right time via an HTTP request.","title":"Scheduled Tasks (Powered by Google Cloud Scheduler)"},{"location":"scheduled_tasks/#defining-a-scheduled-task","text":"You define a scheduled task by inheriting from django_cloud_tasks.tasks.PeriodicTask . The most crucial part is setting the run_every class attribute to a cron expression that defines the schedule. Example: Daily Digest and Hourly Cleanup # In your app's tasks.py (e.g., reports/tasks.py or core/tasks.py) from django_cloud_tasks.tasks import PeriodicTask from django.utils import timezone # from myapp.models import UserActivity, TemporaryFile class GenerateDailyUserActivityDigest(PeriodicTask): # Cron expression: \"At 00:00 on every day-of-week from Monday through Friday.\" # (Assumes server/Cloud Scheduler timezone is UTC by default) run_every = \"0 0 * * 1-5\" def run(self, **kwargs): # kwargs will receive payload from Cloud Scheduler if any is set print(f\"Generating daily user activity digest for {timezone.now().date()}...\") # yesterday = timezone.now().date() - timezone.timedelta(days=1) # activities = UserActivity.objects.filter(timestamp__date=yesterday) # generate_report(activities) # send_report_to_admins() return {\"status\": \"digest_generated\", \"date\": str(timezone.now().date())} class HourlyTemporaryFileCleanup(PeriodicTask): run_every = \"@hourly\" # Cloud Scheduler shorthand for \"0 * * * *\" def run(self, older_than_hours: int = 2, **kwargs): print(f\"Cleaning up temporary files older than {older_than_hours} hours...\") # cutoff_time = timezone.now() - timezone.timedelta(hours=older_than_hours) # TemporaryFile.objects.filter(created_at__lt=cutoff_time).delete() return {\"status\": \"cleanup_done\", \"older_than_hours\": older_than_hours} Key Attributes for PeriodicTask : run_every: str (Required): Defines the schedule using the standard cron format . Cloud Scheduler also supports shorthands like @daily , @hourly , @weekly , @monthly , @yearly . run(**kwargs) : The method containing your task's logic. It's the same as for on-demand tasks. Any JSON payload you define in Cloud Scheduler (or through customization hooks) will be passed as kwargs .","title":"Defining a Scheduled Task"},{"location":"scheduled_tasks/#deploying-scheduled-tasks","text":"Defining the Python class is just the first step. You need to inform Google Cloud Scheduler about your tasks. This is done using a Django management command: python manage.py schedule_tasks What this command does: It scans your Django project for all classes inheriting from PeriodicTask (ensure these modules are imported by Django at startup). It compares these definitions against the jobs currently configured in Google Cloud Scheduler for your DJANGO_CLOUD_TASKS_APP_NAME . It then synchronizes the state: [+] Added : New jobs are created in Cloud Scheduler for tasks found in your code but not yet scheduled. [~] Updated : Existing jobs in Cloud Scheduler are updated if their definition (e.g., cron schedule run_every , target URL, payload) has changed in your code. [-] Deleted : Jobs in Cloud Scheduler are deleted if their corresponding PeriodicTask class has been removed from your code. When to run schedule_tasks ? You should run this command as part of your deployment process, any time you add, remove, or modify the definition (especially run_every or other schedule-related overrides) of a PeriodicTask .","title":"Deploying Scheduled Tasks"},{"location":"scheduled_tasks/#customizing-scheduled-task-behavior","text":"You can override several class methods on your PeriodicTask subclass to fine-tune how it interacts with Google Cloud Scheduler.","title":"Customizing Scheduled Task Behavior"},{"location":"scheduled_tasks/#custom-job-naming-schedule_name","text":"By default, the Cloud Scheduler job name is constructed using your DJANGO_CLOUD_TASKS_APP_NAME , the DJANGO_CLOUD_TASKS_DELIMITER , and the task's class name (e.g., my-app--GenerateDailyUserActivityDigest ). You can customize this for better organization or to adhere to specific naming conventions. class MonthlyBillingRun(PeriodicTask): run_every = \"0 3 1 * *\" # At 03:00 on day-of-month 1. @classmethod def schedule_name(cls) -> str: # Example: \"billing-service-MonthlyBillingRun-prod\" app_name = get_config(\"app_name\") # Fetches DJANGO_CLOUD_TASKS_APP_NAME env = \"prod\" if not settings.DEBUG else \"dev\" return f\"{app_name}-{cls.name()}-{env}\" # cls.name() is the class name def run(self, **kwargs): print(\"Starting monthly billing run...\") # ... billing logic ...","title":"Custom Job Naming (schedule_name)"},{"location":"scheduled_tasks/#custom-http-headers-for-scheduler-requests-schedule_headers","text":"You can inject custom HTTP headers into the requests that Cloud Scheduler makes to your task endpoint. This can be useful for passing specific tokens, versioning information, or routing hints that your Django application or infrastructure might use. class DataSyncWithExternalService(PeriodicTask): run_every = \"0 */6 * * *\" # Every 6 hours @classmethod def schedule_headers(cls) -> dict: # This token might be used by your endpoint's authentication/authorization layer return {\"X-Scheduler-Auth-Token\": \"my-secret-scheduler-token\", \"X-Job-Type\": cls.name()} def run(self, **kwargs): # Your Django view or middleware can inspect request.META for these headers # e.g., request.META.get('HTTP_X_SCHEDULER_AUTH_TOKEN') print(\"Running data sync, triggered with custom headers.\")","title":"Custom HTTP Headers for Scheduler Requests (schedule_headers)"},{"location":"scheduled_tasks/#oidc-authentication-for-scheduler-requests-schedule_use_oidc","text":"Determines if OIDC (OpenID Connect) authentication should be used for the HTTP request from Cloud Scheduler to your application. This is the recommended secure way for Google services to call your services. Default: True . If True , Cloud Scheduler will send a Google-signed OIDC token in the Authorization header. Your application endpoint (usually a Cloud Run service or similar) should be configured to validate this token. If False , no OIDC token is sent by Scheduler. You would rely on other means of securing your endpoint (e.g., network controls, custom schedule_headers with a pre-shared key, though OIDC is preferred). class PublicDataFetcherTask(PeriodicTask): run_every = \"@daily\" @classmethod def schedule_use_oidc(cls) -> bool: # If this task calls a public, non-OIDC protected part of your app, or if auth is handled differently return False # Set to True if your endpoint expects a Google OIDC token def run(self, **kwargs): print(\"Fetching public data...\")","title":"OIDC Authentication for Scheduler Requests (schedule_use_oidc)"},{"location":"scheduled_tasks/#scheduler-level-retry-configuration-schedule_retries","text":"Specifies the number of times Google Cloud Scheduler should retry invoking your task endpoint if the HTTP request fails (e.g., your endpoint returns a 5xx error or doesn't respond). Default: 0 (no retries at the Cloud Scheduler level). This is distinct from any retry logic within your task's run() method or Cloud Tasks queue retries (if the scheduled job itself pushes to a Cloud Tasks queue). class OccasionallyFailingExternalCheck(PeriodicTask): run_every = \"*/30 * * * *\" # Every 30 minutes @classmethod def schedule_retries(cls) -> int: # If the endpoint is down, ask Cloud Scheduler to retry up to 3 times return 3 def run(self, **kwargs): print(\"Performing an external check that might sometimes fail...\") # ... call external service ...","title":"Scheduler-Level Retry Configuration (schedule_retries)"},{"location":"scheduled_tasks/#custom-payload-for-scheduled-tasks","text":"While PeriodicTask.schedule() itself takes **kwargs which are serialized into the payload for Cloud Scheduler, you typically set these arguments directly in the run method's signature with defaults if they are static for the schedule. If you need to dynamically build a more complex payload during the schedule_tasks command or want to override what arguments are sent, you would need to customize the schedule method itself or ensure your run method pulls configuration dynamically. The default arguments sent in the payload by schedule() are simply the **kwargs passed to it. If no kwargs are passed (common for periodic tasks where the logic is self-contained or configured via settings ), the payload will be an empty JSON object {} .","title":"Custom Payload for Scheduled Tasks"},{"location":"scheduled_tasks/#how-it-works-under-the-hood","text":"When PeriodicTask.schedule() is invoked (typically by the schedule_tasks management command), it communicates with Google Cloud Scheduler to create or update a job. This job is configured to: Trigger based on the run_every cron schedule. Make an HTTP POST request to the URL generated by YourTaskClass.url() (this defaults to the standard task execution endpoint in django_cloud_tasks.urls ). Include a JSON payload (by default, an empty dictionary {} unless kwargs were passed to schedule or the task sends a default payload). Use OIDC authentication and custom headers as configured. Essentially, Cloud Scheduler acts as a timed trigger that invokes your PeriodicTask as if it were an on-demand task call arriving at your application's endpoint. Eager Mode Behavior ( DJANGO_CLOUD_TASKS_EAGER = True ) If DJANGO_CLOUD_TASKS_EAGER is true, running python manage.py schedule_tasks will not actually interact with Google Cloud Scheduler. Instead, for any tasks that would have been added or updated, their run() method will be executed synchronously, locally, one time. This is primarily for testing the task logic itself, not the scheduling mechanism. And that's the rundown on keeping your tasks running like clockwork with scheduled tasks!","title":"How It Works Under the Hood"},{"location":"task_field/","text":"TaskField - Storing Task References in Models Django Cloud Tasks provides a custom Django model field, django_cloud_tasks.field.TaskField , designed to store a reference to a specific task class name within your models. This is particularly useful when you want to dynamically determine which task to run based on a model instance's data. Overview The TaskField is a subclass of Django's CharField . It stores the string representation of a task class (e.g., \"MyCoolTask\" or \"myapp.tasks.AnotherTask\" if it's not auto-discoverable by simple name). Key features: Validation: It can automatically validate that the stored string corresponds to an actual task class registered with Django Cloud Tasks. Convenient Class Access: It dynamically adds a property to your model that allows you to directly access the task class itself from the stored name. Usage Here's how you might use TaskField in one of your models: # In your app's models.py from django.db import models from django_cloud_tasks.field import TaskField class NotificationRule(models.Model): name = models.CharField(max_length=100) event_type = models.CharField(max_length=50, unique=True) # Stores the name of the task to execute for this event type task_to_run_name = TaskField() # If you don't want validation at the DB level (e.g. during migrations before tasks are loaded): # task_to_run_name = TaskField(validate_task=False) is_active = models.BooleanField(default=True) def __str__(self): return f\"{self.name} (Event: {self.event_type} -> Task: {self.task_to_run_name})\" def trigger_action(self, payload: dict): if self.is_active and self.task_to_run_name: # Access the actual task class via the dynamically added property # If your field is `foo_name`, the property is `foo_class` ActualTaskClass = self.task_to_run_class if ActualTaskClass: print(f\"Triggering task {ActualTaskClass.name()} for event {self.event_type}\") ActualTaskClass.asap(**payload) else: print(f\"No valid task class found for {self.task_to_run_name}\") In this example: task_to_run_name = TaskField() defines a field that will store the name of a task (e.g., \"SendWelcomeEmailTask\" ). When you save a NotificationRule instance, if validate_task is True (the default), the field will check if the provided string name corresponds to a task discoverable by django-cloud-tasks . A dynamic property task_to_run_class is automatically added to your NotificationRule model. Accessing my_rule.task_to_run_class will return the actual task class (e.g., SendWelcomeEmailTask ), not just its name. Field Options validate_task: bool = True If True (default), the field will validate that the provided task name corresponds to a registered task class when the value is being prepared for the database. This helps ensure data integrity. If False , this validation is skipped. This might be useful in scenarios like during initial data migrations where tasks might not be fully loaded or discoverable yet. max_length: int = 50 The default max_length for the underlying CharField . You can override this if your task names are longer (though 50 characters is usually sufficient for a class name). Other CharField options (like null , blank , default , help_text , etc.) can also be used. How the Class Property Works When you define a field like my_task_field_name = TaskField() , the TaskField automatically adds a new property to your model named my_task_field_class . So, if your field is task_to_run_name , the property becomes task_to_run_class . If your field is backup_task_name , the property becomes backup_task_class . This makes it very convenient to work with, as you can directly call class methods like .asap() , .sync() , or .later() on the retrieved class. Use Cases Configurable Workflows: Allow administrators or users to select different tasks for different events or conditions through a Django admin interface or a settings model. Dynamic Task Routing: In a system that processes various types of jobs, a model instance can hold a reference to the specific task class responsible for handling its job type. Scheduled Task Management: If you have a model that defines custom schedules, it could also store the name of the task to be executed on that schedule. TaskField simplifies storing and retrieving task references in your database, adding a layer of validation and convenience.","title":"TaskField (Model Field)"},{"location":"task_field/#taskfield-storing-task-references-in-models","text":"Django Cloud Tasks provides a custom Django model field, django_cloud_tasks.field.TaskField , designed to store a reference to a specific task class name within your models. This is particularly useful when you want to dynamically determine which task to run based on a model instance's data.","title":"TaskField - Storing Task References in Models"},{"location":"task_field/#overview","text":"The TaskField is a subclass of Django's CharField . It stores the string representation of a task class (e.g., \"MyCoolTask\" or \"myapp.tasks.AnotherTask\" if it's not auto-discoverable by simple name). Key features: Validation: It can automatically validate that the stored string corresponds to an actual task class registered with Django Cloud Tasks. Convenient Class Access: It dynamically adds a property to your model that allows you to directly access the task class itself from the stored name.","title":"Overview"},{"location":"task_field/#usage","text":"Here's how you might use TaskField in one of your models: # In your app's models.py from django.db import models from django_cloud_tasks.field import TaskField class NotificationRule(models.Model): name = models.CharField(max_length=100) event_type = models.CharField(max_length=50, unique=True) # Stores the name of the task to execute for this event type task_to_run_name = TaskField() # If you don't want validation at the DB level (e.g. during migrations before tasks are loaded): # task_to_run_name = TaskField(validate_task=False) is_active = models.BooleanField(default=True) def __str__(self): return f\"{self.name} (Event: {self.event_type} -> Task: {self.task_to_run_name})\" def trigger_action(self, payload: dict): if self.is_active and self.task_to_run_name: # Access the actual task class via the dynamically added property # If your field is `foo_name`, the property is `foo_class` ActualTaskClass = self.task_to_run_class if ActualTaskClass: print(f\"Triggering task {ActualTaskClass.name()} for event {self.event_type}\") ActualTaskClass.asap(**payload) else: print(f\"No valid task class found for {self.task_to_run_name}\") In this example: task_to_run_name = TaskField() defines a field that will store the name of a task (e.g., \"SendWelcomeEmailTask\" ). When you save a NotificationRule instance, if validate_task is True (the default), the field will check if the provided string name corresponds to a task discoverable by django-cloud-tasks . A dynamic property task_to_run_class is automatically added to your NotificationRule model. Accessing my_rule.task_to_run_class will return the actual task class (e.g., SendWelcomeEmailTask ), not just its name.","title":"Usage"},{"location":"task_field/#field-options","text":"validate_task: bool = True If True (default), the field will validate that the provided task name corresponds to a registered task class when the value is being prepared for the database. This helps ensure data integrity. If False , this validation is skipped. This might be useful in scenarios like during initial data migrations where tasks might not be fully loaded or discoverable yet. max_length: int = 50 The default max_length for the underlying CharField . You can override this if your task names are longer (though 50 characters is usually sufficient for a class name). Other CharField options (like null , blank , default , help_text , etc.) can also be used.","title":"Field Options"},{"location":"task_field/#how-the-class-property-works","text":"When you define a field like my_task_field_name = TaskField() , the TaskField automatically adds a new property to your model named my_task_field_class . So, if your field is task_to_run_name , the property becomes task_to_run_class . If your field is backup_task_name , the property becomes backup_task_class . This makes it very convenient to work with, as you can directly call class methods like .asap() , .sync() , or .later() on the retrieved class.","title":"How the Class Property Works"},{"location":"task_field/#use-cases","text":"Configurable Workflows: Allow administrators or users to select different tasks for different events or conditions through a Django admin interface or a settings model. Dynamic Task Routing: In a system that processes various types of jobs, a model instance can hold a reference to the specific task class responsible for handling its job type. Scheduled Task Management: If you have a model that defines custom schedules, it could also store the name of the task to be executed on that schedule. TaskField simplifies storing and retrieving task references in your database, adding a layer of validation and convenience.","title":"Use Cases"}]}